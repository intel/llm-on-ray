name: Finetune on Intel GPU

on:
  workflow_call:
    inputs:
      runner_container_image:
        type: string
        default: '10.1.2.13:5000/llmray-build'
      http_proxy:
        type: string
        default: 'http://proxy-chain.intel.com:911'
      https_proxy:
        type: string
        default: 'http://proxy-chain.intel.com:911'

jobs:
  finetune:
    name: finetune on gpu test
    strategy:
      matrix:
        model: [ pythia-6.9b, gpt-j-6b ]
    runs-on: self-hosted

    defaults:
      run:
        shell: bash
    container:
      image: ${{ vars.ACTIONS_RUNNER_CONTAINER_IMAGE }}
      env:
        http_proxy: ${{ vars.HTTP_PROXY_CONTAINER }}
        https_proxy: ${{ vars.HTTPS_PROXY_CONTAINER }}
      volumes:
        - /var/run/docker.sock:/var/run/docker.sock

    steps:
      - name: Checkout
        uses: actions/checkout@v2

      - name: Running task on Intel GPU
        run: |
          rm ~/borealis-runner/llm-on-ray.tar.gz -f
          tar zcf ~/borealis-runner/llm-on-ray.tar.gz -C ~/actions-runner/_work/llm-on-ray .
          cd ~/borealis-runner/
          python3 finetune_on_pvc.py --base_model "${{ matrix.model }}"
      - name: Test Summary
        run: echo "to be continued"