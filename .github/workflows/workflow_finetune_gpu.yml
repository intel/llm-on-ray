name: Finetune-GPU

on:
  workflow_call:
    inputs:
      runner_container_image:
        type: string
        default: '10.1.2.13:5000/llmray-build'
      http_proxy:
        type: string
        default: 'http://10.24.221.169:912'
      https_proxy:
        type: string
        default: 'http://10.24.221.169:912'

jobs:
  finetune-gpu:
    name: finetune-gpu
    strategy:
      matrix:
        model: [ NousResearch/Llama-2-7b-chat-hf ]
    runs-on: self-hosted

    defaults:
      run:
        shell: bash
    container:
      image: ${{ inputs.runner_container_image }}
      env:
        http_proxy: ${{ inputs.http_proxy }}
        https_proxy: ${{ inputs.https_proxy }}
        SHELL: bash -eo pipefail
      volumes:
        - /var/run/docker.sock:/var/run/docker.sock

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Running task on Intel GPU
        run: |
          rm /home/ci/borealis-runner/llm-on-ray.tar.gz -f
          tar zcf /home/ci/borealis-runner/llm-on-ray.tar.gz -C /home/ci/actions-runner/_work/llm-on-ray .
          cd /home/ci/borealis-runner/
          python3 finetune_on_pvc.py --need_create_conda_env true --base_models "${{ matrix.model }}"

