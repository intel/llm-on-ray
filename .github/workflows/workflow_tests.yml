name: tests

on:
  workflow_call:
    inputs:
      ci_type:
        type: string
        default: 'pr'
      runner_container_image:
        type: string
        default: '10.1.2.13:5000/llmray-build'
      http_proxy:
        type: string
        default: 'http://proxy-chain.intel.com:911'
      https_proxy:
        type: string
        default: 'http://proxy-chain.intel.com:911'
      runner_config_path:
        type: string
        default: '/home/ci/llm-ray-actions-runner'
      code_checkout_path:
        type: string
        default: '/home/ci/actions-runner/_work/llm-on-ray/llm-on-ray'
      model_cache_path:
        type: string
        default: '/mnt/DP_disk1/huggingface/cache'

jobs:
  ut-test-runner:
    name : utils-test
    runs-on: ubuntu-latest
        
    defaults:
      run:
        shell: bash
      

    steps:
      - uses: actions/checkout@v4
      - name: Set up Python
        # This is the version of the action for setting up Python, not the Python version.
        uses: actions/setup-python@v4
        with:
          # Semantic version range syntax or exact version of a Python version
          python-version: '3.9'
          # Optional - x64 or x86 architecture, defaults to x64
          architecture: 'x64'
      # You can test your matrix by printing the current Python version
      - name: Display Python version
        run: python -c "import sys; print(sys.version)"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r ./tests/requirements.txt

      - name: Start tests-utils
        run: |
          bash -c "./tests/run_test.sh"

      - name: Test Summary
        run: echo "to be continued"
      
  pr-test-runner:
    name: predicter-test
    strategy:
      matrix:
        model: [ gpt-j-6b, gpt2, bloom-560m, opt-125m, mpt-7b, mistral-7b-v0.1, mpt-7b-bigdl, neural-chat-7b-v3-1 ]
        isPR:
          - ${{inputs.ci_type == 'pr'}}

        exclude:
          - { isPR: true }

        include:
          - { model: "gpt2"}
          - { model: "mistral-7b-v0.1"}
          # - { model: "mpt-7b-bigdl"}


    runs-on: self-hosted

    defaults:
      run:
        shell: bash
    container:
      image: ${{ inputs.runner_container_image }}
      env:
        http_proxy: ${{ inputs.http_proxy }}
        https_proxy: ${{ inputs.https_proxy }}
      volumes:
        - /var/run/docker.sock:/var/run/docker.sock
    
    steps:
      - name: Checkout
        uses: actions/checkout@v2
      
      - name: Determine Target
        id: "target"
        run: |
          target="inference"
          if [[ ${{ matrix.model }} == "mpt-7b-bigdl" ]]; then
            target="${target}_bigdl_cpu"
          fi
          echo "target is ${target}"
          echo "target=$target" >> $GITHUB_OUTPUT

      - name: Build Docker Image
        run: |
          if [[ ${{ matrix.model }} == "mpt-7b-bigdl" ]]; then
            DF_SUFFIX=".bigdl-cpu"
          else
            DF_SUFFIX=".cpu_and_deepspeed"
          fi
          TARGET=${{steps.target.outputs.target}}
          docker build ./ --build-arg CACHEBUST=1 --build-arg http_proxy=${{ inputs.http_proxy }} --build-arg https_proxy=${{ inputs.https_proxy }} -f dev/docker/Dockerfile${DF_SUFFIX} -t ${TARGET}:latest && yes | docker container prune && yes
          docker image prune -f

      - name: Start Docker Container
        run: |
          TARGET=${{steps.target.outputs.target}}
          cid=$(docker ps -q --filter "name=${TARGET}")
          if [[ ! -z "$cid" ]]; then docker stop $cid && docker rm $cid; fi
          # check and remove exited container
          cid=$(docker ps -a -q --filter "name=${TARGET}")
          if [[ ! -z "$cid" ]]; then docker rm $cid; fi
          docker run -tid -v ${{ inputs.model_cache_path }}:/root/.cache/huggingface/hub -v ${{ inputs.code_checkout_path }}:/root/llm-on-ray -e http_proxy=${{ inputs.http_proxy }} -e https_proxy=${{ inputs.https_proxy }} --name="${TARGET}" --hostname="${TARGET}-container" ${TARGET}:latest
         
      - name: Run predicter Test
        run: |
            TARGET=${{steps.target.outputs.target}}
            docker exec "${TARGET}" bash -c "pip install pytest"
            docker exec "${TARGET}" bash -c "./tests/run_predicter_test.sh"
           
      - name: Stop Container
        if: success() || failure()
        run: |
          TARGET=${{steps.target.outputs.target}}
          cid=$(docker ps -q --filter "name=${TARGET}")
          if [[ ! -z "$cid" ]]; then docker stop $cid && docker rm $cid; fi

      - name: Test Summary
        run: echo "to be continued"           
  