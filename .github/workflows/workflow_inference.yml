name: Inference

on:
  workflow_call:
    inputs:
      ci_type:
        type: string
        default: 'pr'
      runner_container_image:
        type: string
        default: '10.1.2.13:5000/llmray-build'
      http_proxy:
        type: string
        default: 'http://proxy-chain.intel.com:911'
      https_proxy:
        type: string
        default: 'http://proxy-chain.intel.com:911'
      runner_config_path:
        type: string
        default: '/home/ci/llm-ray-actions-runner'
      code_checkout_path:
        type: string
        default: '/home/ci/actions-runner/_work/llm-on-ray/llm-on-ray'
      model_cache_path:
        type: string
        default: '/mnt/DP_disk1/huggingface/cache'

concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref }}-inf
  cancel-in-progress: true

jobs:
  inference:
    name: inference test
    strategy:
      matrix:
        # for mistral-7b-v0.1, we use bigdl-cpu to verify
        model: [ gpt-j-6b, gpt2, bloom-560m, opt-125m, mpt-7b, mistral-7b-v0.1, mpt-7b-bigdl, neural-chat-7b-v3-1, llama-2-7b-chat-hf ]
        isPR:
          - ${{inputs.ci_type == 'pr'}}

        exclude:
          - { isPR: true }

        include:
          - { model: "gpt-j-6b"}
          - { model: "mistral-7b-v0.1"}
          - { model: "mpt-7b-bigdl"}
          - { model: "llama-2-7b-chat-hf-vllm"}
          - dtuner_model: nathan0/mpt-7b-deltatuner-model
            model: mpt-7b

    runs-on: self-hosted

    defaults:
      run:
        shell: bash
    container:
      image: ${{ vars.ACTIONS_RUNNER_CONTAINER_IMAGE }}
      env:
        http_proxy: ${{ vars.HTTP_PROXY_CONTAINER }}
        https_proxy: ${{ vars.HTTPS_PROXY_CONTAINER }}
      volumes:
        - /var/run/docker.sock:/var/run/docker.sock

    steps:
      - name: Checkout
        uses: actions/checkout@v2

      - name: Set Name Prefix
        id: "prefix"
        run: |
          target="inference"
          if [[ ${{ matrix.model }} == "mpt-7b-bigdl" ]]; then
            target="${target}_bigdl_cpu"
          elif [[ ${{ matrix.model }} == "llama-2-7b-chat-hf-vllm" ]]; then
            target="${target}_vllm"
          fi
          echo "target is ${target}"
          echo "target=$target" >> $GITHUB_OUTPUT

      - name: Build Docker Image
        run: |
          if [[ ${{ matrix.model }} == "mpt-7b-bigdl" ]]; then
            DF_SUFFIX=".bigdl-cpu"
          elif [[ ${{ matrix.model }} == "llama-2-7b-chat-hf-vllm" ]]; then
            DF_SUFFIX=".vllm"
          elif [[ ${{ matrix.model }} == "gpt-j-6b" ]]; then
            DF_SUFFIX=".cpu_and_deepspeed.pip_non_editable"
          else
            DF_SUFFIX=".cpu_and_deepspeed"
          fi
          PREFIX=${{steps.prefix.outputs.prefix}}
          docker build ./ --build-arg CACHEBUST=1 --build-arg http_proxy=${{ vars.HTTP_PROXY_CONTAINER }} --build-arg https_proxy=${{ vars.HTTPS_PROXY_CONTAINER }} -f dev/docker/Dockerfile${DF_SUFFIX} -t ${PREFIX}:latest && yes | docker container prune && yes
          docker image prune -f

      - name: Start Docker Container
        run: |
          TARGET=${{steps.target.outputs.target}}
          cid=$(docker ps -q --filter "name=${TARGET}")
          if [[ ! -z "$cid" ]]; then docker stop $cid && docker rm $cid; fi
          docker run -tid -v ${{ vars.MODEL_CACHE_PATH }}:/root/.cache/huggingface/hub -v ${{ vars.CODE_CHECKOUT_PATH }}:/root/llm-on-ray -e http_proxy=${{ vars.HTTP_PROXY_CONTAINER }} -e https_proxy=${{ vars.HTTPS_PROXY_CONTAINER }} --name="${PREFIX}" --hostname="${PREFIX}-container" ${PREFIX}:latest

      - name: Start Ray Cluster
        run: |
          TARGET=${{steps.target.outputs.target}}
          docker exec "${TARGET}" bash -c "./dev/scripts/start-ray-cluster.sh"

      - name: Run Inference Test
        run: |
          TARGET=${{steps.target.outputs.target}}
          if [[ ${{ matrix.model }} == "mpt-7b-bigdl" ]]; then
            docker exec "${TARGET}" bash -c "python inference/serve.py --config_file inference/models/bigdl/mpt-7b-bigdl.yaml --simple"
          elif [[ ${{ matrix.model }} == "llama-2-7b-chat-hf-vllm" ]]; then
            docker exec "${TARGET}" bash -c "python inference/serve.py --config_file .github/workflows/config/llama-2-7b-chat-hf-vllm-fp32.yaml --simple"
          else
            docker exec "${TARGET}" bash -c "python inference/serve.py --simple --models ${{ matrix.model }}"
          fi
          echo Non-streaming query:
          docker exec "${TARGET}" bash -c "python examples/inference/api_server_simple/query_single.py --model_endpoint http://127.0.0.1:8000/${{ matrix.model }}"
          echo Streaming query:
          docker exec "${TARGET}" bash -c "python examples/inference/api_server_simple/query_single.py --model_endpoint http://127.0.0.1:8000/${{ matrix.model }} --streaming_response"

      - name: Run Inference Test with Deltatuner
        if: ${{ matrix.dtuner_model }}
        run: |
          TARGET=${{steps.target.outputs.target}}
          docker exec "${TARGET}" bash -c "python inference/serve.py --config_file .github/workflows/config/mpt_deltatuner.yaml --simple"
          docker exec "${TARGET}" bash -c "python examples/inference/api_server_simple/query_single.py --model_endpoint http://127.0.0.1:8000/${{ matrix.model }}"
          docker exec "${TARGET}" bash -c "python examples/inference/api_server_simple/query_single.py --model_endpoint http://127.0.0.1:8000/${{ matrix.model }} --streaming_response"

      - name: Run Inference Test with DeepSpeed
        run: |
          TARGET=${{steps.target.outputs.target}}
          if [[ ${{ matrix.model }} =~ ^(gpt2|falcon-7b|mpt-7b.*)$ ]]; then
            echo ${{ matrix.model }} is not supported!
          else
            docker exec "${PREFIX}" bash -c "python .github/workflows/config/update_inference_config.py --config_file inference/models/\"${{ matrix.model }}\".yaml --output_file \"${{ matrix.model }}\".yaml.deepspeed --deepspeed"
            docker exec "${PREFIX}" bash -c "KEEP_SERVE_TERMINAL='false' python inference/run_model_serve.py --config_file \"${{ matrix.model }}\".yaml.deepspeed"
            docker exec "${PREFIX}" bash -c "python inference/run_model_infer.py --num_iter 1 --model_endpoint http://127.0.0.1:8000/${{ matrix.model }}"
            docker exec "${PREFIX}" bash -c "python inference/run_model_infer.py --num_iter 1 --model_endpoint http://127.0.0.1:8000/${{ matrix.model }} --streaming_response"
          fi

      - name: Run Inference Test with DeepSpeed and Deltatuner
        if: ${{ matrix.dtuner_model }}
        run: |
          TARGET=${{steps.target.outputs.target}}
          if [[ ${{ matrix.model }} =~ ^(gpt2|mpt-7b.*)$ ]]; then
            echo ${{ matrix.model }} is not supported!
          else
            docker exec "${TARGET}" bash -c "python inference/serve.py --config_file .github/workflows/config/mpt_deltatuner_deepspeed.yaml --simple"
            docker exec "${TARGET}" bash -c "python examples/inference/api_server_simple/query_single.py --model_endpoint http://127.0.0.1:8000/${{ matrix.model }}"
            docker exec "${TARGET}" bash -c "python examples/inference/api_server_simple/query_single.py --model_endpoint http://127.0.0.1:8000/${{ matrix.model }} --streaming_response"
          fi

      - name: Run Inference Test with REST API
        run: |
          TARGET=${{steps.target.outputs.target}}
          if [[ ${{ matrix.model }} == "mpt-7b-bigdl" ]]; then
            docker exec "${TARGET}" bash -c "python inference/serve.py --config_file inference/models/bigdl/mpt-7b-bigdl.yaml"
          elif [[ ! ${{ matrix.model }} == "llama-2-7b-chat-hf-vllm" ]]; then
            docker exec "${TARGET}" bash -c "python inference/serve.py --models ${{ matrix.model }}"
            docker exec "${TARGET}" bash -c "python examples/inference/api_server_openai/query_http_requests.py --model_name ${{ matrix.model }}"
          fi

      - name: Stop Ray
        run: |
          TARGET=${{steps.target.outputs.target}}
          cid=$(docker ps -q --filter "name=${TARGET}")
          if [[ ! -z "$cid" ]]; then
            docker exec "${TARGET}" bash -c "ray stop"
          fi

      - name: Stop Container
        if: success() || failure()
        run: |
          TARGET=${{steps.target.outputs.target}}
          cid=$(docker ps -q --filter "name=${TARGET}")
          if [[ ! -z "$cid" ]]; then docker stop $cid && docker rm $cid; fi

      - name: Test Summary
        run: echo "to be continued"
