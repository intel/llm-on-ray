name: Finetune

on:
  workflow_call:
    inputs:
      ci_type:
        type: string
        default: 'pr'
      runner_container_image:
        type: string
        default: '10.1.2.13:5000/llmray-build'
      http_proxy:
        type: string
        default: 'http://10.24.221.149:911'
      https_proxy:
        type: string
        default: 'http://10.24.221.149:911'
      runner_config_path:
        type: string
        default: '/home/ci/llm-ray-actions-runner'
      code_checkout_path:
        type: string
        default: '/home/ci/actions-runner/_work/llm-on-ray/llm-on-ray'
      model_cache_path:
        type: string
        default: '/mnt/DP_disk1/huggingface/cache'

concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref }}-ft
  cancel-in-progress: true

jobs:
  inference:
    name: finetune test
    strategy:
      matrix:
        model: [ EleutherAI/gpt-j-6b, meta-llama/Llama-2-7b-chat-hf, gpt2, bigscience/bloom-560m, facebook/opt-125m, mosaicml/mpt-7b-chat, huggyllama/llama-7b, mistralai/Mistral-7B-v0.1 ]
        isPR:
          - ${{inputs.ci_type == 'pr'}}

        exclude:
          - { isPR: true }
        include:
          - { model: "EleutherAI/gpt-j-6b"}
          - { model: "meta-llama/Llama-2-7b-chat-hf"}
          - { model: "mistralai/Mistral-7B-v0.1"}

    runs-on: self-hosted

    defaults:
      run:
        shell: bash
    container:
      image: ${{ inputs.runner_container_image }}
      env:
        http_proxy: ${{ inputs.http_proxy }}
        https_proxy: ${{ inputs.https_proxy }}
      volumes:
        - /var/run/docker.sock:/var/run/docker.sock
        - ${{ inputs.runner_config_path }}:/root/actions-runner-config

    steps:
      - name: Checkout
        uses: actions/checkout@v2

      - name: Load environment variables
        run: cat /root/actions-runner-config/.env >> $GITHUB_ENV

      - name: Build Docker Image
        run: |
          docker build ./ --build-arg CACHEBUST=1 --build-arg http_proxy=${{ inputs.http_proxy }} --build-arg https_proxy=${{ inputs.https_proxy }} -f dev/docker/Dockerfile.cpu_and_deepspeed -t finetune:latest && yes | docker container prune && yes
          docker image prune -f

      - name: Start Docker Container
        run: |
          cid=$(docker ps -q --filter "name=finetune")
          if [[ ! -z "$cid" ]]; then docker stop $cid && docker rm $cid; fi
          # check and remove exited container
          cid=$(docker ps -a -q --filter "name=finetune")
          if [[ ! -z "$cid" ]]; then docker rm $cid; fi
          docker run -tid -v ${{ inputs.model_cache_path }}:/root/.cache/huggingface/hub -v ${{ inputs.code_checkout_path }}:/root/llm-on-ray -e http_proxy=${{ inputs.http_proxy }} -e https_proxy=${{ inputs.https_proxy }} --name="finetune" --hostname="finetune-container" finetune:latest

      - name: Run Finetune Test
        run: |
          docker exec "finetune" bash -c "source \$(python -c 'import oneccl_bindings_for_pytorch as torch_ccl;print(torch_ccl.cwd)')/env/setvars.sh; RAY_SERVE_ENABLE_EXPERIMENTAL_STREAMING=1 ray start --head --node-ip-address 127.0.0.1 --ray-debugger-external; RAY_SERVE_ENABLE_EXPERIMENTAL_STREAMING=1  ray start --address='127.0.0.1:6379' --ray-debugger-external"
          CMD=$(cat << EOF
          import yaml
          conf_path = "finetune/finetune.yaml"
          with open(conf_path, encoding="utf-8") as reader:
              result = yaml.load(reader, Loader=yaml.FullLoader)
              result['General']['base_model'] = "${{ matrix.model }}"
              if "${{ matrix.model }}" == "mosaicml/mpt-7b-chat":
                  result['General']['config']['trust_remote_code'] = True
              else:
                  result['General']['config']['trust_remote_code'] = False
              if "${{ matrix.model }}" == "EleutherAI/gpt-j-6b" or "${{ matrix.model }}" == "gpt2":
                  result['General']["gpt_base_model"] = True
              else:
                  result['General']["gpt_base_model"] = False
              if "${{ matrix.model }}" == "meta-llama/Llama-2-7b-chat-hf":
                  result['General']["config"]["use_auth_token"] = "${{ env.HF_ACCESS_TOKEN }}"
              else:
                  result['General']["config"]["use_auth_token"] = None
              result['Training']['epochs'] = 1
              if "${{ matrix.model }}" == "gpt2":
                  # to verify oneccl
                  result['Training']['num_training_workers'] = 2
              else:
                  result['Training']['num_training_workers'] = 1
              result['General']['lora_config'] = None
          with open(conf_path, 'w') as output:
              yaml.dump(result, output, sort_keys=False)
          EOF
          )
          docker exec "finetune" python -c "$CMD"
          docker exec "finetune" bash -c "python finetune/finetune.py --config_file finetune/finetune.yaml"

      - name: Run PEFT-LoRA Test
        run: |
          docker exec "finetune" bash -c "rm -rf /tmp/llm-ray/*"
          CMD=$(cat << EOF
          import yaml
          conf_path = "finetune/finetune.yaml"
          with open(conf_path, encoding="utf-8") as reader:
              result = yaml.load(reader, Loader=yaml.FullLoader)
              result['General']['lora_config'] = {
                  "task_type": "CAUSAL_LM",
                  "r": 8,
                  "lora_alpha": 32,
                  "lora_dropout": 0.1
              }
              if "${{ matrix.model }}" == "mistralai/Mistral-7B-v0.1":
                  result['General']['lora_config']['target_modules'] = ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj", "lm_head",]
              else:
                  result['General']['lora_config']['target_modules'] = None
          with open(conf_path, 'w') as output:
              yaml.dump(result, output, sort_keys=False)
          EOF
          )
          docker exec "finetune" python -c "$CMD"
          docker exec "finetune" bash -c "python finetune/finetune.py --config_file finetune/finetune.yaml"

      - name: Run Deltatuner Test on DENAS-LoRA Model
        run: |
          if [[ ${{ matrix.model }} =~ ^(mosaicml\/mpt-7b-chat|huggyllama\/llama-7b|meta-llama\/Llama-2-7b-chat-hf|mistralai\/Mistral-7B-v0.1)$ ]]; then
            echo ${{ matrix.model }} is not supported!
          else
            docker exec "finetune" bash -c "rm -rf /tmp/llm-ray/*"
            CMD=$(cat << EOF
          import os
          import yaml
          os.system("cp -r $(python -m pip show deltatuner | grep Location | cut -d: -f2)/deltatuner/conf/best_structure examples/")
          conf_path = "finetune/finetune.yaml"
          with open(conf_path, encoding="utf-8") as reader:
              result = yaml.load(reader, Loader=yaml.FullLoader)
              result['General']['lora_config'] = {
                  "task_type": "CAUSAL_LM",
                  "r": 8,
                  "lora_alpha": 32,
                  "lora_dropout": 0.1
              }
              result['General']['deltatuner_config'] = {
                  "algo": "lora",
                  "denas": True,
                  "best_model_structure": f"examples/best_structure/${{ matrix.model }}-best_structure.jsonl",
              }
          with open(conf_path, 'w') as output:
              yaml.dump(result, output, sort_keys=False)
          EOF)
            docker exec "finetune" python -c "$CMD"
            docker exec "finetune" bash -c "python finetune/finetune.py --config_file finetune/finetune.yaml"
          fi

      - name: Stop Ray
        run: |
          cid=$(docker ps -q --filter "name=finetune")
          if [[ ! -z "$cid" ]]; then
            docker exec "finetune" bash -c "ray stop"
          fi

      - name: Stop Container
        if: success() || failure()
        run: |
          cid=$(docker ps -q --filter "name=finetune")
          if [[ ! -z "$cid" ]]; then docker stop $cid && docker rm $cid; fi

      - name: Test Summary
        run: echo "to be continued"
