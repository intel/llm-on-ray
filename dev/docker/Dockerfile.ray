# syntax=docker/dockerfile:1
FROM anyscale/ray:2.10.0-py39

# Define build arguments
ARG DOCKER_NAME=default
ARG PYPJ=default
ENV LANG C.UTF-8

USER root

WORKDIR /root/

# setup env
SHELL ["/bin/bash", "--login", "-c"]

RUN conda install -y -c conda-forge gxx=12.3 gxx_linux-64=12.3 libxcrypt

RUN git clone https://github.com/intel/llm-on-ray.git

WORKDIR /root/llm-on-ray

# Used to invalidate docker build cache with --build-arg CACHEBUST=$(date +%s)
ARG CACHEBUST=1

# Install dependencies
RUN --mount=type=cache,target=/root/.cache/pip pip install -e .[${PYPJ}] --extra-index-url https://download.pytorch.org/whl/cpu \
    --extra-index-url https://pytorch-extension.intel.com/release-whl/stable/cpu/us/

# Use shell scripting to conditionally install packages
RUN if [ "${DOCKER_NAME}" = ".cpu_and_deepspeed" ]; then ds_report && /tmp/install-oneapi.sh;fi
RUN if [ "${DOCKER_NAME}" = ".ipex-llm" ]; then /tmp/install-oneapi.sh; fi
RUN if [ "${DOCKER_NAME}" = ".vllm" ]; then  ./install-vllm-cpu.sh; fi

COPY ./dev/scripts/entrypoint_user.sh /entrypoint.sh
RUN chmod +x /entrypoint.sh

ENTRYPOINT ["/entrypoint.sh"]
