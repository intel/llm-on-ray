From 1df9ba8d085f55d5141cdbe0857987dec12f1f7b Mon Sep 17 00:00:00 2001
From: yuanwu <yuan.wu@intel.com>
Date: Fri, 8 Dec 2023 04:53:13 +0000
Subject: [PATCH] Add the Huggingface tokenizer

Signed-off-by: yuanwu <yuan.wu@intel.com>
---
 .../Megatron-DeepSpeed/megatron/arguments.py  |  6 +-
 .../megatron/tokenizer/tokenizer.py           | 86 +++++++++++++++++++
 2 files changed, 90 insertions(+), 2 deletions(-)

diff --git a/PyTorch/nlp/DeepSpeedExamples/Megatron-DeepSpeed/megatron/arguments.py b/PyTorch/nlp/DeepSpeedExamples/Megatron-DeepSpeed/megatron/arguments.py
index b9861fa0..516c2abb 100644
--- a/PyTorch/nlp/DeepSpeedExamples/Megatron-DeepSpeed/megatron/arguments.py
+++ b/PyTorch/nlp/DeepSpeedExamples/Megatron-DeepSpeed/megatron/arguments.py
@@ -871,8 +871,10 @@ def _add_data_args(parser):
                                 'BertWordPieceCase',
                                 'GPT2BPETokenizer',
                                 'SentencePieceTokenizer',
-                                'LlamaTokenizer'],
+                                'HFTokenizer'],
                        help='What type of tokenizer to use.')
+    group.add_argument('--tokenizer-model', type=str, default=None,
+                       help='Sentencepiece tokenizer model.')
     group.add_argument('--data-impl', type=str, default='infer',
                        choices=['lazy', 'cached', 'mmap', 'infer'],
                        help='Implementation of indexed datasets.')
@@ -1174,4 +1176,4 @@ def _add_hpu_optimizations_args(parser):
                        action='store_true',
                        help='Flatten operands of linear layers what yields better performance')
 
-    return parser
\ No newline at end of file
+    return parser
diff --git a/PyTorch/nlp/DeepSpeedExamples/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py b/PyTorch/nlp/DeepSpeedExamples/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py
index e4a49306..7989be48 100644
--- a/PyTorch/nlp/DeepSpeedExamples/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py
+++ b/PyTorch/nlp/DeepSpeedExamples/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py
@@ -18,6 +18,7 @@
 from abc import ABC
 from abc import abstractmethod
 
+from transformers import AutoTokenizer
 from .bert_tokenization import FullTokenizer as FullBertTokenizer
 from .gpt2_tokenization import GPT2Tokenizer
 from .sentencepiece_tokenization import SentencePieceTokenizer
@@ -47,6 +48,10 @@ def build_tokenizer(args):
     elif args.tokenizer_type == 'SentencePieceTokenizer':
         assert args.tokenizer_model_file is not None
         tokenizer = _SentencePieceTokenizer(args.tokenizer_model_file, args.tokenizer_eod_id)
+    elif args.tokenizer_type == 'HFTokenizer':
+        assert args.tokenizer_model is not None
+        tokenizer = _HFTokenizer(args.tokenizer_model)
+
     else:
         raise NotImplementedError('{} tokenizer is not '
                                   'implemented.'.format(args.tokenizer_type))
@@ -328,3 +333,84 @@ class _SentencePieceTokenizer(AbstractTokenizer):
     @property
     def eod(self):
         return self.eod_id
+
+class _HFTokenizer(AbstractTokenizer):
+    """HF Tokenizer"""
+    def __init__(self, tokenizer_name_or_path):
+        name = tokenizer_name_or_path
+        super().__init__(name)
+        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path)
+        self.encoder = self.tokenizer.get_vocab()
+        self.decoder = {v: k for k, v in self.encoder.items()}
+
+    @property
+    def vocab_size(self):
+        return self.tokenizer.vocab_size
+
+    @property
+    def vocab(self):
+        return self.encoder
+
+    @property
+    def inv_vocab(self):
+        return self.decoder
+
+    def tokenize(self, text):
+        return self.tokenizer.encode(text)
+
+    def detokenize(self, token_ids):
+        return self.tokenizer.decode(token_ids)
+
+    @property
+    def bos(self):
+        return self.bos_token_id
+
+    @property
+    def bos_token_id(self):
+        candidate = self.tokenizer.eos_token_id
+        return self._check_token_candidate(candidate)
+
+    @property
+    def cls(self):
+        candidate = self.tokenizer.cls_token_id
+        return self._check_token_candidate(candidate)
+
+    @property
+    def sep(self):
+        candidate = self.tokenizer.sep_token_id
+        return self._check_token_candidate(candidate)
+
+    @property
+    def pad(self):
+        candidate = self.tokenizer.pad_token_id
+        return self._check_token_candidate(candidate)
+
+    @property
+    def eod(self):
+        candidate = self.tokenizer.eos_token_id
+        return self._check_token_candidate(candidate)
+
+    @property
+    def eos(self):
+        return self.eos_token_id
+
+    @property
+    def eos_token_id(self):
+        candidate = self.tokenizer.eos_token_id
+        return self._check_token_candidate(candidate)
+
+    @property
+    def mask(self):
+        candidate = self.tokenizer.mask_token_id
+        return self._check_token_candidate(candidate)
+
+    @property
+    def additional_special_tokens_ids(self):
+        return self.tokenizer.additional_special_tokens_ids
+
+    @staticmethod
+    def _check_token_candidate(candidate):
+        """Checks whether the candidate is None or not, and raises an exception if it is."""
+        if candidate is None:
+            raise AttributeError("Requested token doesn't exist in current tokenizer")
+        return candidate
-- 
2.25.1

