{
    # Mode of the task, two option [ray|standalone].
    "run_mode" : "ray",
    # The global seed of pytorch.
    "seed": 42,
    # The global threads num of pytorch.
    "torch_thread_num": 56,
    # Config of accelerator, all items will be transfered to accelerate.Accelerator().
    
    "initializer":{
        "type":"MegatronInitializer",
        "name": "megatron",
        "megatron_config": {
            "data_path": ["/home/user/workspace/data/my-gpt2_text_document"],
            "data_impl": "mmap",
            "micro_batch_size": 1,
            "global_batch_size": 8,
            "seq_length": 2048,
            "use_dataset_only": True,
            "vocab_file": "/home/user/workspace/data/gpt2-vocab.json",
            "tokenizer_type": "GPT2BPETokenizer",
            "merge_file": "/home/user/workspace/data/gpt2-merges.txt",
            "train_iters": 300,
            "eval_interval": 10,
            "split": "949,50,1",
        },
    },

 
    "datasets": {
        "type": "MegatronDataset",
    },

    "trainer": {
        # The type of trainer, now only DefaultTrainer is supported.
        "type": "HuggingFacePreTrainer",
        # Number of train epochs
        "model": {
            # The type of model, now only HuggingfaceModelForCausalLM is supported.
            #"type": "HuggingFaceModelForCausalLM",
            "type": "HuggingFaceModelFromConfig", 
            # The name/path of model in huggingface.
            "name": "huggyllama/llama-7b",
            # Config of model, all items will be transfered to AutoModelForCausalLM.from_pretrained().
            "config": {
                "torch_dtype": "bfloat16"
            }
        },

        "tokenizer": {
            # The type of dataset, now only HuggingfaceTokenizer is supported.
            "type": "HuggingFaceTokenizer",
            # The name/path of tokenizer in huggingface.
            "name": "huggyllama/llama-7b",
            # Config of tokenizer, all items will be transfered to transformers.AutoTokenizer.from_pretrained().
            "config": {
            }
        },

        "training_config": {
            "per_device_train_batch_size": 1,
            "per_device_eval_batch_size": 1,
            "do_train": True,
            "do_eval": True,
            "save_strategy": "no",
            "output_dir": "/tmp/hf_trainer/",
            "gaudi_config_name": "Habana/gpt2",
            "use_habana": True,
            "throughput_warmup_steps": 3,
            "use_lazy_mode": True,
            "overwrite_output_dir": True,
            "max_steps": 300,
            "seed": 42,
            "bf16": True,
            "deepspeed":{
                "steps_per_print": 64,
                "train_batch_size": 8,
                "train_micro_batch_size_per_gpu": 1,
                "gradient_accumulation_steps": "auto",
                "bf16": {
                    "enabled": True
                },
                "gradient_clipping": 1.0,
                "zero_optimization": {
                    "stage": 3,
                    "overlap_comm": False,
                    "reduce_scatter": False,
                    "contiguous_gradients": False
                }
            },
        },
        "dataprocesser": {
            # The type of dataprocesser. 
            "type": "MegatronProcesser",
        },
    },
    # Ray related configuration, Only used when mode is set to ray
    "ray_config": {
        # The config of ray.init. All items will be tranfered to ray.init().
        # More information can refer to https://docs.ray.io/en/latest/ray-core/api/doc/ray.init.html
        "init": {
            # Environment variables for ray workers
            "runtime_env": {
                "env_vars": {
                    "OMP_NUM_THREADS": "56", 
                    #"ACCELERATE_USE_CPU": "True", 
                    "ACCELERATE_MIXED_PRECISION": "no",
                    #"CCL_WORKER_COUNT": "1",        # CCL setting
                    #"CCL_LOG_LEVEL": "info",
                    "WORLD_SIZE": "8",    # Enable multi-process
                }
            },
            # The address of the Ray cluster to connect to.
            "address": "auto",
            # The IP address of the node that we are on.
            "_node_ip_address": "127.0.0.1",
        },
        "scaling_config": {
            # Number of worker.
            "num_workers": 8,
            # The amount of resources per worker.
            "resources_per_worker": {
                "CPU": 10,
            },
            # The placement strategy to use for the placement group of the Ray actors.
            "placement_strategy": "SPREAD"
        },
        "torch_config": {
            # The backend of the communication library.
            "backend" : "hccl",
        },
        "failure_config": {
            # The maximum number of restarts when task fail.
            "max_failures": 0
        },
        "run_config": {
            # Local dir to save training results to.
            "local_dir": "/tmp/llm-ray"
        }
    }
}
