# I am python, not json
{
    # Mode of the task, two option [ray|standalone].
    "run_mode" : "ray",
    # The global seed of pytorch.
    "seed": 42,
    # The global threads num of pytorch.
    "torch_thread_num": 56,
    "initializer":{
        "type":"MegatronInitializer",
        "name": "megatron",
        "megatron_config": {
            "num_layers": 24,
            "hidden_size":2048,
            "ffn_hidden_size": 5504,
            "num_attention_heads": 16,
            "max_position_embeddings":2048,
            "num_key_value_heads": 4,
            "vocab_file": "/home/user/workspace/workspace/llm-ray/Finetune/gpt2-vocab.json",
            "tokenizer_type": "GPT2BPETokenizer",
            "merge_file": "/home/user/workspace/workspace/llm-ray/Finetune/gpt2-merges.txt",
            "seq_length": 2048,
            "micro_batch_size": 4,
            #"eval_micro_batch_size": 2,
            #"rampup_batch_size": "16 16 5_000_000",
            "global_batch_size": 32,
            "train_samples": 300_000_000,
            "loss_scale": 12,
            "clip_grad": 1.0,
            "seed": 42,
            "optimizer": "adam",
            "adam_beta1": 0.9,
            "adam_beta2":0.999,
            "adam_eps": 1e-8,
            "lr": 3e-4,
            "min_lr": 3e-5,
            "lr_decay_style": "cosine",
            #"lr_decay_samples": 126_953_125,
            #"lr_warmup_samples": 216_320,
            #"lr_warmup_iters": 2000,
            "clip_grad": 1.0,
            "weight_decay": 0.1,
            "log_interval": 10,
            "save_interval": 1500,
            "eval_interval": 1000,
            "eval_iters": 5,
            "DDP_impl": "local",
            "tensor_model_parallel_size": 2,
            "pipeline_model_parallel_size": 2,
            #"codecarbon_dir": "/home/user/workspace/workspace/codecarbon",
            #"tensorboard_dir": "/home/user/workspace/workspace/tensorboard",
            #"tensorboard_queue_size": 5,
            "log_timers_to_tensorboard": True,
            "log_batch_size_to_tensorboard": True,
            "log_validation_ppl_to_tensorboard": True,
            "fp16": True,
            "apply_query_key_layer_scaling": True,
            "attention_dropout": 0,
            "hidden_dropout": 0,
            "use_rotary_position_embeddings": True,
            "untie_embeddings_and_output_weights": True,
            "swiglu": True,
            "normalization": "rmsnorm",
            "add_bias_linear": True,
            "deepspeed": True,
            "deepspeed_config": {
                "train_micro_batch_size_per_gpu": 4,
                "train_batch_size": 32,
                "steps_per_print": 1,
                #"gradient_clipping": 1.0,
                "zero_optimization": {
                    "stage": 0
                },
                "fp16": {
                    "enabled": True,
                    #"loss_scale": 0,
                    #"loss_scale_window": 500,
                    #"hysteresis": 2,
                    #"min_loss_scale": 1,
                    #"initial_scale_power": 12
                },
                #"steps_per_print": 2000,
                "wall_clock_breakdown": False
            },
            "zero_stage":0,
            "deepspeed_activation_checkpointing": True,
            "save": "/home/user/workspace/workspace/checkpoint_megatron",
            "data_path": ["/home/user/workspace/workspace/my-gpt2_text_document"],
            "data_impl": "mmap",
            "split": "949,50,1",
            "distributed_backend": "nccl"
        },
    },

    "trainer": {
        # The type of trainer, now only DefaultTrainer is supported.
        "use_hpu": False,
        "type": "MegatronDeepspeedPreTrainer",
        # Output directory. Only absolute path is supported.
        "output": "/tmp/output",
        "checkpoint_step": 100,
        "checkpoint": {
            # The root path of checkpoint. Only absolute path is supported
            "root_path": "/tmp/llm-ray/checkpoint",
        }
    },
    # Ray related configuration, Only used when mode is set to ray
    "ray_config": {
        # The config of ray.init. All items will be tranfered to ray.init().
        # More information can refer to https://docs.ray.io/en/latest/ray-core/api/doc/ray.init.html
        "init": {
            # Environment variables for ray workers
            "runtime_env": {
                "env_vars": {
                    "OMP_NUM_THREADS": "128", 
                    #"ACCELERATE_USE_CPU": "True", 
                    "ACCELERATE_MIXED_PRECISION": "no",
                    #"CCL_WORKER_COUNT": "2",        # CCL setting
                    #"CCL_LOG_LEVEL": "info",
                    "WORLD_SIZE": "8",    # Enable multi-process
                    "CUDA_DEVICE_MAX_CONNECTIONS": "1",
                    "MAX_JOBS": "96",
                }
            },
            # The address of the Ray cluster to connect to.
            "address": "auto",
            # The IP address of the node that we are on.
            "_node_ip_address": "127.0.0.1",
        },
        "scaling_config": {
            # Number of worker.
            "num_workers": 8,
            "use_gpu": True,
            # The amount of resources per worker.
            "resources_per_worker": {
                "CPU": 10,
                "GPU": 1
            },
            # The placement strategy to use for the placement group of the Ray actors.
            "placement_strategy": "SPREAD"
        },
        "torch_config": {
            # The backend of the communication library.
            "backend" : "nccl",
        },
        "failure_config": {
            # The maximum number of restarts when task fail.
            "max_failures": 0
        },
        "run_config": {
            # Local dir to save training results to.
            "local_dir": "/tmp/llm-ray"
        }
    }
}
