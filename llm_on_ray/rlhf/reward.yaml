General:
  base_model: EleutherAI/gpt2
  output_dir: /tmp/llm-ray/output/rm
  checkpoint_dir: /tmp/llm-ray/checkpoint/rm
Dataset:
  train_file: examples/data/sample_rm_data.jsonl
  validation_file: null
  validation_split_percentage: 5
Training:
  optimizer: AdamW
  batch_size: 2
  epochs: 3
  learning_rate: 1.0e-05
  lr_scheduler: linear
  weight_decay: 0.0
  num_training_workers: 2
  resources_per_worker:
    CPU: 32
