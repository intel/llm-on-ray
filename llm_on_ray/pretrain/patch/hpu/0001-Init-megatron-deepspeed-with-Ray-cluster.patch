From 28adc503428cac3b547ed6190ab1026233431a57 Mon Sep 17 00:00:00 2001
From: yuanwu <yuan.wu@intel.com>
Date: Mon, 4 Sep 2023 09:01:09 +0000
Subject: [PATCH 1/2] Init megatron-deepspeed with Ray cluster

Signed-off-by: yuanwu <yuan.wu@intel.com>
---
 .../Megatron-DeepSpeed/megatron/arguments.py  | 10 +-
 .../megatron/global_vars.py                   | 10 +-
 .../Megatron-DeepSpeed/megatron/initialize.py | 92 ++++++++++---------
 .../Megatron-DeepSpeed/megatron/training.py   | 14 ++-
 4 files changed, 71 insertions(+), 55 deletions(-)

diff --git a/PyTorch/nlp/DeepSpeedExamples/Megatron-DeepSpeed/megatron/arguments.py b/PyTorch/nlp/DeepSpeedExamples/Megatron-DeepSpeed/megatron/arguments.py
index b9861fa0..4438dca6 100644
--- a/PyTorch/nlp/DeepSpeedExamples/Megatron-DeepSpeed/megatron/arguments.py
+++ b/PyTorch/nlp/DeepSpeedExamples/Megatron-DeepSpeed/megatron/arguments.py
@@ -26,7 +26,7 @@ from megatron.enums import PositionEmbeddingType
 
 
 def parse_args(extra_args_provider=None, defaults={},
-               ignore_unknown_args=False):
+               ignore_unknown_args=False, external_args={}):
     """Parse all arguments."""
     parser = argparse.ArgumentParser(description='Megatron-LM Arguments',
                                      allow_abbrev=False)
@@ -67,6 +67,11 @@ def parse_args(extra_args_provider=None, defaults={},
     else:
         args = parser.parse_args()
 
+    for key in external_args:
+        if key in args:
+            setattr(args, key, external_args[key])
+
+
     # helper argument to set deepspeed pipeline parallel or not
     args.ds_pipeline_enabled = not args.no_pipeline_parallel
 
@@ -238,7 +243,8 @@ def parse_args(extra_args_provider=None, defaults={},
     else:
         assert args.encoder_seq_length is not None
         args.seq_length = args.encoder_seq_length
-
+    if isinstance(args.position_embedding_type, str):
+        args.position_embedding_type = PositionEmbeddingType[args.position_embedding_type]
     if args.position_embedding_type == PositionEmbeddingType.absolute \
             or args.position_embedding_type == PositionEmbeddingType.alibi \
             or args.position_embedding_type == PositionEmbeddingType.learnable:
diff --git a/PyTorch/nlp/DeepSpeedExamples/Megatron-DeepSpeed/megatron/global_vars.py b/PyTorch/nlp/DeepSpeedExamples/Megatron-DeepSpeed/megatron/global_vars.py
index aba3f3d0..deb2acba 100644
--- a/PyTorch/nlp/DeepSpeedExamples/Megatron-DeepSpeed/megatron/global_vars.py
+++ b/PyTorch/nlp/DeepSpeedExamples/Megatron-DeepSpeed/megatron/global_vars.py
@@ -90,11 +90,12 @@ def get_timers():
 
 
 def set_global_variables(extra_args_provider=None, args_defaults={},
-                         ignore_unknown_args=False):
+                         ignore_unknown_args=False, external_args={}):
     """Set args, tokenizer, tensorboard-writer, adlr-autoresume, and timers."""
     args = _parse_args(extra_args_provider=extra_args_provider,
                        defaults=args_defaults,
-                       ignore_unknown_args=ignore_unknown_args)
+                       ignore_unknown_args=ignore_unknown_args,
+                       external_args=external_args)
     _build_num_microbatches_calculator(args)
     if args.vocab_file or args.tokenizer_model_file:
         _ = _build_tokenizer(args)
@@ -104,13 +105,14 @@ def set_global_variables(extra_args_provider=None, args_defaults={},
 
 
 def _parse_args(extra_args_provider=None, defaults={},
-                ignore_unknown_args=False):
+                ignore_unknown_args=False, external_args={}):
     """Parse entire arguments."""
     global _GLOBAL_ARGS
     _ensure_var_is_not_initialized(_GLOBAL_ARGS, 'args')
     _GLOBAL_ARGS = parse_args(extra_args_provider=extra_args_provider,
                               defaults=defaults,
-                              ignore_unknown_args=ignore_unknown_args)
+                              ignore_unknown_args=ignore_unknown_args,
+                              external_args=external_args)
     return _GLOBAL_ARGS
 
 
diff --git a/PyTorch/nlp/DeepSpeedExamples/Megatron-DeepSpeed/megatron/initialize.py b/PyTorch/nlp/DeepSpeedExamples/Megatron-DeepSpeed/megatron/initialize.py
index 22452f5b..649be991 100644
--- a/PyTorch/nlp/DeepSpeedExamples/Megatron-DeepSpeed/megatron/initialize.py
+++ b/PyTorch/nlp/DeepSpeedExamples/Megatron-DeepSpeed/megatron/initialize.py
@@ -38,7 +38,7 @@ import deepspeed
 import deepspeed.utils.groups as groups
 
 def initialize_megatron(extra_args_provider=None, args_defaults={},
-                        ignore_unknown_args=False, allow_no_cuda=False):
+                        ignore_unknown_args=False, allow_no_cuda=False, external_args={}):
     """Set global variables, initialize distributed, and
     set autoresume and random seeds.
     `allow_no_cuda` should not be set unless using megatron for cpu only
@@ -47,12 +47,11 @@ def initialize_megatron(extra_args_provider=None, args_defaults={},
     Returns a function to finalize distributed env initialization
     (optionally, only when args.lazy_mpu_init == True)
     """
-
     # Parse args, build tokenizer, and set adlr-autoresume,
     # tensorboard-writer, and timers.
     set_global_variables(extra_args_provider=extra_args_provider,
                          args_defaults=args_defaults,
-                         ignore_unknown_args=ignore_unknown_args)
+                         ignore_unknown_args=ignore_unknown_args, external_args=external_args)
 
     args = get_args()
     if os.getenv("P2P_DUMMY_MODE_PHASE") != "2":
@@ -214,53 +213,15 @@ def _initialize_distributed():
                   'skipping initialization ...', flush=True)
         args.rank = torch.distributed.get_rank()
         args.world_size = torch.distributed.get_world_size()
+        if args.deepspeed or args.ds_inference:
+            deepspeed.init_distributed(dist_backend=args.distributed_backend)
+
 
     else:
         print("_initialize_distributed: Initializing with below params:")
         print("args.local_rank:", args.local_rank)
         print("args.world_size:", args.world_size)
         print("args.rank:", args.rank)
-        # TODO SW-65249 need to align behavior between device types
-        device_count = None
-        print("args.distributed_backend:", args.distributed_backend)
-        if args.distributed_backend == 'hccl':
-            import habana_frameworks.torch as htcore
-            device_count = htcore.hpu.device_count()
-            if args.hpu_deterministic:
-                assert args.use_hpu, f"--hpu-deterministic supported only with --use-hpu flag"
-                htcore.hpu.setDeterministic(True)
-            print("hccl device_count: ", device_count)
-        elif args.distributed_backend == 'nccl':
-            device_count = torch.cuda.device_count()
-        elif args.distributed_backend == 'gloo':
-            # no limit of devices when working on CPU, setting 8.
-            device_count = int(os.getenv('GPUS_PER_NODE', '8'))
-        else:
-            assert False, f"Unsupported backend {args.distributed_backend}"
-
-        # Manually set the device ids.
-        if device_count > 0:
-            device = args.rank % device_count
-            if args.local_rank is not None:
-                assert args.local_rank == device, \
-                    'expected local-rank to be the same as rank % device-count.'
-            else:
-                args.local_rank = device
-        else:
-            assert False, "Error: device_count is not positive"
-
-        if args.distributed_backend == 'hccl':
-            device = torch.device('hpu')
-        elif args.distributed_backend == 'nccl':
-            torch.cuda.set_device(device)
-            device = torch.device('cuda')
-        elif args.distributed_backend == 'gloo':
-            device = torch.device('cpu')
-        else:
-            assert False, f"Unsupported backend {args.distributed_backend}"
-
-        args.device = device
-
         if args.rank == 0:
             print('> initializing torch distributed ...', flush=True)
 
@@ -279,6 +240,49 @@ def _initialize_distributed():
                 backend=args.distributed_backend,
                 world_size=args.world_size, rank=args.rank,
                 init_method=init_method)
+
+    # TODO SW-65249 need to align behavior between device types
+    device_count = None
+    print("args.distributed_backend:", args.distributed_backend)
+    if args.distributed_backend == 'hccl':
+        import habana_frameworks.torch as htcore
+        device_count = htcore.hpu.device_count()
+        if args.hpu_deterministic:
+            assert args.use_hpu, f"--hpu-deterministic supported only with --use-hpu flag"
+            htcore.hpu.setDeterministic(True)
+        print("hccl device_count: ", device_count)
+    elif args.distributed_backend == 'nccl':
+        device_count = torch.cuda.device_count()
+    elif args.distributed_backend == 'gloo':
+        # no limit of devices when working on CPU, setting 8.
+        device_count = int(os.getenv('GPUS_PER_NODE', '8'))
+    else:
+        assert False, f"Unsupported backend {args.distributed_backend}"
+
+    # Manually set the device ids.
+    if device_count > 0:
+        device = args.rank % device_count
+        if args.local_rank is not None:
+            assert args.local_rank == device, \
+                'expected local-rank to be the same as rank % device-count.'
+        else:
+            args.local_rank = device
+    else:
+        assert False, "Error: device_count is not positive"
+
+    if args.distributed_backend == 'hccl':
+        device = torch.device('hpu')
+    elif args.distributed_backend == 'nccl':
+        torch.cuda.set_device(device)
+        device = torch.device('cuda')
+    elif args.distributed_backend == 'gloo':
+        device = torch.device('cpu')
+    else:
+        assert False, f"Unsupported backend {args.distributed_backend}"
+
+    args.device = device
+
+
     # Set the tensor model-parallel, pipeline model-parallel, and
     # data-parallel communicators.
     if device_count > 0:
diff --git a/PyTorch/nlp/DeepSpeedExamples/Megatron-DeepSpeed/megatron/training.py b/PyTorch/nlp/DeepSpeedExamples/Megatron-DeepSpeed/megatron/training.py
index 8f8e4451..63b4cd40 100644
--- a/PyTorch/nlp/DeepSpeedExamples/Megatron-DeepSpeed/megatron/training.py
+++ b/PyTorch/nlp/DeepSpeedExamples/Megatron-DeepSpeed/megatron/training.py
@@ -97,7 +97,8 @@ def pretrain(train_valid_test_dataset_provider,
              model_provider,
              forward_step_func,
              extra_args_provider=None,
-             args_defaults={}):
+             args_defaults={},
+             external_args={}):
     """Main training program.
 
     This function will run the followings in the order provided:
@@ -123,8 +124,8 @@ def pretrain(train_valid_test_dataset_provider,
     """
 
     # Initalize and get arguments, timers, and Tensorboard writer.
-    initialize_megatron(extra_args_provider=extra_args_provider,
-                        args_defaults=args_defaults)
+    initialize_megatron(extra_args_provider=extra_args_provider, ignore_unknown_args=True,
+                        args_defaults=args_defaults, external_args=external_args)
 
     args = get_args()
 
@@ -149,8 +150,11 @@ def pretrain(train_valid_test_dataset_provider,
     timers = get_timers()
 
     if args.deepspeed:
-        args.deepspeed_configuration = json.load(
-            open(args.deepspeed_config, 'r', encoding='utf-8'))
+        if isinstance(args.deepspeed_config, dict) :
+            args.deepspeed_configuration = args.deepspeed_config
+        else:
+            args.deepspeed_configuration = json.load(
+                open(args.deepspeed_config, 'r', encoding='utf-8'))
         if "curriculum_learning" in args.deepspeed_configuration and \
             "enabled" in args.deepspeed_configuration["curriculum_learning"]:
             args.curriculum_learning = args.deepspeed_configuration[ \
-- 
2.25.1

