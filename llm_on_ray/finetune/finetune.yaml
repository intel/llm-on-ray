General:
  trainer_type: "dpo"
  base_model: mistralai/Mistral-7B-v0.1
  gpt_base_model: false
  output_dir: /tmp/llm-ray/output
  save_strategy: no
  config:
    trust_remote_code: false
    use_auth_token: null
  enable_gradient_checkpointing: false
Dataset:
  data_type: "intel_orca"
  train_file: "Intel/orca_dpo_pairs"
  group: true
  max_length: 512
  block_size: 512
  shuffle: false
  validation_file: null
  validation_split_percentage: 5
  max_prompt_length: 512
  pad_max: false
Training:
  optimizer: adamw_hf
  batch_size: 1
  epochs: 3
  learning_rate: 1.0e-05
  lr_scheduler: cosine
  weight_decay: 0.0
  mixed_precision: bf16
  device: cpu
  num_training_workers: 1
  resources_per_worker:
    CPU: 32
  accelerate_mode: DDP
  gradient_accumulation_steps: 1
  logging_steps: 10
  deepspeed_config_file: finetune/ds_config_zero2.json
  use_dpo: True
  beta: 0.0
