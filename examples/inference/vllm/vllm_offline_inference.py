#
# Copyright 2023 The LLM-on-Ray Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

from vllm import LLM, SamplingParams

from vllm.extension import ns as ns

from time import perf_counter


############################
# TODO: TODO: TODO: reset request_id and seq_id before they reach to max value of int64_t
############################

# Sample prompts.
# prompts = [
#     "Hello, my name is",
#     "The president of the United States is",
#     "The capital of France is",
#     "The future of AI is",
#     "Hello, my name is",
#     "The president of the United States is",
#     "The capital of France is",
#     "The future of AI is",
#     "Hello, my name is",
#     "The president of the United States is",
#     "The capital of France is",
#     "The future of AI is",
# ]
prompts = [
    "Russell Brunson's Perfect Webinar Script is a framework for delivering webinars that are designed to sell a product or service. ",
    "Tony Robbins describes six core human needs that drive our behaviors and motivations. These six needs are:\n\n1. Certainty: The need for safety, stability, and predictability.",
    "1. Homogeneity: The segment should consist of customers who share similar characteristics and behaviors.\n2. Distinctiveness: The segment should be different from other segments in terms of their characteristics and behaviors.\n3. Stability: The segment should remain relatively stable over time and not change drastically. The characteristics and behaviors of customers within the segment should not change significantly.",
    'In Java, I want to replace string like "This is a new {object} at {place}" with a Map, {object: "student", "point 3, 4"}, and get a result "This is a new student at point 3, 4". How can I do?',
    'You can use the `String.format()` method in Java to replace placeholders in a string with values from a map. Here\'s an example code snippet that demonstrates how you can achieve this:\n```java\nimport java.util.HashMap;\nimport java.util.Map;\n\npublic class StringReplaceExample {\n    public static void main(String[] args) {\n        String input = "This is a new {object} at {place}";\n        Map<String, String> replacements = new HashMap<>();\n        replacements.put("object", "student");\n        replacements.put("place", "point 3, 4");\n\n ',
    'The language used to describe the addressing modes of these instructions is metaphorical and grandiose, emphasizing the complexity and power of these commands. For example, the use of "enigmatic" and "confounding" to describe JMP ABCD and MOV AX, [BX+SI], respectively, suggests that these instructions are not easily understood and require a level of expertise to comprehend.\n\nSimilarly, the use of "inscrutable" and "cryptic" to describe MOV AX, [100] and MOV AX, [BX], respectively, implies that these commands are shrouded in mystery and are difficult to decipher. The speaker\'s use of "perplexing" and "unfathomable" to describe MOV AX, [BX\\*2+SI] and MOV AX, BX, respectively, ',
    "Lo and behold! The arcane and elusive art of metaphorical language has been summoned forth to expound upon the enigmatic addressing modes of the instructions at hand. The speakers have wielded grandiose expressions with utmost reverence and awe, extolling the ineffable power and bewildering functionality of these directives. Among the inscrutable commands are the confounding JMP ABCD, the abstruse MOV AX, [BX+SI], the unfathomable MOV AX, [100],",
    "more more perplexity and verbose",
    "By the grace of the gods,",
]
# Create a sampling params object.
# sampling_params = SamplingParams(temperature=0, top_p=1, max_tokens=256, use_beam_search=True, best_of=2)
# sampling_params = SamplingParams(temperature=0.6, top_p=0.95, max_tokens=256, top_k=40)
sampling_params = SamplingParams(max_tokens=512)
# sampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=256)

#######################################
prompts = [
    "Hello, how are you?",
    "What is your name?",
    "What is your favorite color?",
    "What is your favorite food?",
    "What is your favorite movie?",
    "What is your favorite song?",
    "What is your favorite book?",
    "What is your favorite animal",
    "What is your favorite sport?",
    "What is your favorite hobby?",
    "What is your favorite subject?",
    "What is your favorite game?",
    "What is your favorite TV show?",
    "What is your favorite actor?",
    "What is your favorite actress?",
    "What is your favorite singer?",
    "What is your favorite band?",
    "What is your favorite artist?",
    "What is your favorite author?",
    "What is your favorite poet?",
]
# 32 input
prompts = [
    "Tony Robbins describes six core human needs that drive our behaviors and motivations. These six needs are:\n\n1. Certainty: The need for safety, stability, and predictability."
]
# 1024 input
prompts = [
    "It is done, and submitted. You can play 'Survival of the Tastiest' on Android, and on the web. Playing on the web works, but you have to simulate multiple touch for table moving and that can be a bit confusing. There is a lot I'd like to talk about. I will go through every topic, insted of making the typical what went right/wrong list. Concept Working over the theme was probably one of the hardest tasks which I had to face. Originally, I had an idea of what kind of game I wanted to develop, gameplay wise - something with a lot of enemies/actors, simple graphics, maybe set in space, controlled from a top-down view. I was confident that I could fit any theme around it. In the end, the problem with a theme like 'Evolution' in a game is that evolution is unassisted. It happens through several seemingly random mutations over time, with the most apt permutation surviving. This genetic car simulator is, in my opinion, a great example of actual evolution of a species facing a challenge. But is it a game? In a game, you need to control something to reach an objective. That control goes against what evolution is supposed to be like. If you allow the user to pick how to evolve something, it's not evolution anymore - it's the equivalent of intelligent design, the fable invented by creationists to combat the idea of evolution. Being agnostic and a Pastafarian, that's not something that rubbed me the right way. Hence, my biggest dillema when deciding what to create was not with what I wanted to create, but with what I did not. I didn't want to create an 'intelligent design' simulator and wrongly call it evolution. This is a problem, of course, every other contestant also had to face. And judging by the entries submitted, not many managed to work around it. I'd say the only real solution was through the use of artificial selection, somehow. So far, I have not seen any entry using this at its core gameplay. Alas, this is just a fun competition and after a while I decided not to be as strict with the game idea, and allowed myself to pick whatever I thought would work out. My initial idea was to create something where humanity tried to evolve to a next level but had some kind of foe trying to stop them from doing so. I kind of had this image of human souls flying in space towards a monolith or a space baby (all based in 2001: A Space Odyssey of course) but I couldn't think of compelling (read: serious) mechanics for that. Borgs were my next inspiration, as their whole hypothesis fit pretty well into the evolution theme. But how to make it work? Are you the borg, or fighting the Borg? The third and final idea came to me through my girlfriend, who somehow gave me the idea of making something about the evolution of Pasta. The more I thought about it the more it sounded like it would work, so I decided to go with it. Conversations with my inspiring co-worker Roushey (who also created the 'Mechanical Underdogs' signature logo for my intros) further matured the concept, as it involved into the idea of having individual pieces of pasta flying around and trying to evolve until they became all-powerful. A secondary idea here was that the game would work to explain how the Flying Spaghetti Monster came to exist - by evolving from a normal dinner table. So the idea evolved more or less into this: you are sitting a table. You have your own plate, with is your 'base'. There are 5 other guests at the table, each with their own plate. Your plate can spawn little pieces of pasta. You do so by 'ordering' them through a menu. Some pastas are better than others; some are faster, some are stronger. They have varying 'costs', which are debited from your credits (you start with a number of credits). Once spawned, your pastas start flying around. Their instinct is to fly to other plates, in order to conquer them (the objective of the game is having your pasta conquer all the plates on the table). But they are really autonomous, so after being spawned, you have no control over your pasta (think DotA or LoL creeps). Your pasta doesn't like other people's pasta, so if they meet, they shoot sauce at each other until one dies. You get credits for other pastas your own pasta kill. Once a pasta is in the vicinity of a plate,"
]
# simple
# prompts = [
#     "You are a highly skilled AI trained in language comprehension and summarization. I would like you to read the following transcription of a meeting and summarize it into a concise abstract paragraph. Aim to retain the most important points, providing a coherent and readable summary that could help a person understand the main points of the discussion without needing to read the entire text. Please avoid unnecessary details or tangential points. list the people involved in the converation, that way that will scan transcribe for you, yeah. Kyle Short: Well then I GPT did the transcription. Vinay Kumar: Yes. Chakri Krishna: We will, absolutely. Chakri Krishna: So you wanna go? Kyle Short: Alright. Chakri Krishna: You wanted to be intros, Kyle. Kyle Short: Yeah, I think at least for Jeevan and Rich, your faces will be new to Shredder. Vinay Kumar: I know them. I've been. I've talked to both of them. Kyle Short: You've got to do the one on ones already. Excellent. Rich Rodgers: Yeah. Yep. Jason Rawlins: Yep. Kyle Short: OK, then we can skip it. Awesome. Chakri Krishna: Alright, cool. Chakri Krishna: Let's do white you know, Kyle, to kind of do an extension of what you were doing with security. Chakri Krishna: Is IT security or is IT security? Kyle Short: They confirmed it as security. Chakri Krishna: Security. Thank you very much. We'll, we'll throw. I'll throw a whiteboard out there. Chakri Krishna: At least this whole kind of help me and maybe we can like draw a little bit better. Chakri Krishna: On what we're building. Chakri Krishna: So I'm not, I'm not 100% sure where to start to make it make most sense. Chakri Krishna: Sure. Chakri Krishna: Alright, cool. Yeah, yeah. Cool. Vinay Kumar: Start from I GPT, we know I GPT right and then yeah. Chakri Krishna: Oops, I've got my pen stuck to the mouse. Kyle Short: And just so you know, we spent the whole 4 hours I spent with security. Today was basically on the one diagram that you had which was your high-level"
# ]
# medium
# prompts = [
# "You are a highly skilled AI trained in language comprehension and summarization.  Please avoid unnecessary details or tangential points. list the people involved in the converation presentation and create notes for each slide that I can use to guide my presentation.  Each time you see next slide, this is an indication that a new slide is being presented and you should create a new section of notes.  If you see the text “for example” this indicates that I am giving an important example that is critical to the context of the presentation.  Always include notes that prompt me to remember the examples I am providing.  Provide a bolded word “Example” at the beginning of these types of examples.  Act as an expert presenter who is guiding a novice presenter on how to prepare notes for a presentation that will be given to a large audience.  The presenter will be nervous and might need additional notes to help keep him on track.  Ask me questions before generating notes that might help me in creating a good presentation.  Only produce one set of notes for one slide at a time.  Ask me if I am done with the notes or want to work on them more for each slide.  When I tell you I am done with the notes for the slide, you will move to the next slide where we will work on the next slide’s notes.  DICTATION: I want to start out by talking about the journey the generative AI journey that we see from an IT perspective I have a timeline here listed out and kind of goes through a little bit before 2023 out to the future and we'll cover those things as they let's see our our kind of go go through those things um as we all know there are a lot of industry innovations that took place over the last many years actually even decades when we think about generative AI large language models these concepts have been around a long time decades but we haven't really had the compute power in place to leverage them like we are these days which is wonderful from a timeline perspective I'm starting here in November of 2023 November of 2023 open AI releases their product called ChatGPT and as we all know the media picks it up a lot of things start happening people start using it you know people are declaring it's the the end of work or you know a lot of different things that that happen you know new new technology comes out and it's all every everyone buzzes about it the interesting thing that we see is that there's suddenly lots and lots of money being poured in billions 10s of billions of dollars being poured into these ventures for open AI for for different solutions that are out there and huge vendors some of our primary customers start to develop solutions that that are are also available so they start to give sort of copy cat solutions if you will or similar ones right one thing that is important also note the servant for me is a software engineer one of the key things that changes here is that open AI opens up their models to be used via an API So what this does for me is a software engineer as it changes the picture a little bit I now I'm not a data scientist but now I can start building solutions around these APIs and there's a lot of software engineers out there and so you see an explosion of all kinds of different things that are out there software engineers are assembling things putting things together and really kind of innovating with what we can do with these APIs and that's a big change around that same time kind of going duck back down onto the bottom of this picture here we at Intel start to kick off all kinds of pocs proofs of concepts around the company how can we make use of these models of large language models generative AI what does it look like and when I say hundreds of PCs POC's kicked off they did hundreds of different POC's and STG leadership asked IT to kind of get their arms around what's happening and understand the use cases which we did we launched off a work group and we called it degenerative a work group probably some of you here were part of that and we we split it up into a couple of different ways but we're really focused on trying to understand the use cases and how those use cases are realized inside of Intel and what they do right I'll along the process as we're doing POC's within the company the industry is marching along as well you know Intel releases a product called articulate and spun it off into a different company you see big improvements from the capabilities and open source models as well as other vendors get involved like you know you've got Google out there and and and they're and they're associated with partners and a WS and so a lot of lot of innovation happening in this space as well all at the same time billions of dollars being funneled into this business and inside of Intel again we launched something called the AI inside Intel MRC the IRC and that that ELT level team starts to develop some goals um those goals kind of come down to to me and my team in various bits and pieces I know their their scope is vast but they they launch and they give us some specific goals which we've been marching towards so the kind of if you look at this picture there's a lot of you know a lot of pocs happening a lot of work happening a lot of innovation happening in the company and at the same time we're we're given a directive by the IRC to figure out how we can focus on to a single platform all of the innovation that's been done across the company and bring it together and bring it down into one platform that can be leveraged by Intel and Intel users and Intel developers in order to build solutions and figure out what that is so that's kind of what we have been doing as from an IT perspective looking at the use cases trying to figure out a strategy doing our own proofs of concept and of course we've built some tools in IT as well and really kind of narrowing down that that those use cases so we can build a single platform for people to use looking out in the future beyond 2024 we do see the ability and at the end towards the end of 2024 to start onboarding more and more folks onto a single platform and I'll talk about that what that means and uh and and kind of leveraging that platform all along this journey and I'll mention this a couple of times right we have to kind of figure out how do we how do we leverage the Intel hardware and the Intel programs around the Habana gaudi accelerators around AI PC how do we really make that a true game changer for the industry and for us as well right we need to be using our own tools so really what we do what we did from this journey perspective is take all of the innovation on try to that's happening so many teams are doing so many cool things try to understand what's happening figure out how it can service the use cases from a common perspective and really kind of take the company forward OK we'll go to the next slide During the the work group that we ran as I mentioned before we were looking for common use cases what are the primary use cases now as you know the use cases for generative are vast but they kind of fall into three buckets if you will one kind of around developer productivity with a big focus on code generation another one kind of around the QA bots with a focus on rag or retrieval augmented generation and the last one is just general productivity with focus around prompt engineering and the capabilities there we we looked at the code generation side of things early on and we were we found a couple different patterns there were there were people who were using the tools like you know the the GPT tools to just generate code have it do analysis of code and things like that and sort of in a separate window and teams like the MSO team were building VS code plugins to kind of really test and figure out how just the open AI models themselves would perform for us in terms of generating code and analyzing code at the same time there were several tools out in the industry that were really built into the IDE itself and allowed developers to kind of inline work with their code and have code suggestions there obviously GitHub copilot is one of the primary ones and we are doing a pilot with that right now in in the across the organization and and trying to figure out how that actually works but during this sort of common use case discovery people are building all kinds of different things there as well so you know how do you do test case generation how do you do inject this into your cicd pipeline to maybe create documentation I've seen some good use cases there as well so a lot of stuff around developer productivity and this extends beyond just the coder right so so there are some great use cases being done as well around you know creating other types of code assets as well beyond just like software that you would use in an IT perspective right how do we create product how do we do those types of things so a lot of really interesting things being done there that was a huge focus the the next around retrieval augmented generation this is a very common use case that we see across a lot of different folks and the the focus for a lot of people when we were doing the work group was really trying to figure out the right way to approach rag and the technical pieces that we needed right to to enable it as well as you know where where should we take it in the future so so during the work group people who've who've used this we got a we have our our dbas platform that's up and running and it's using a plugin for PostgreSQL called PG vector so folks can can start using a rag based model with PG vector and that's all supported standardized T stuff right now and that's one of the things we worked on out there but also at the same time a lot of innovation people really working on improving the the results that come back from that from the rag model as well as you know trying to figure out and fine tune exactly how to to build the different prompt workflows there as well so also the last column there on prompt engineering of early round general productivity so this this we see a lot of work or saw a lot of work people are just really interested in prompt engineering and trying to figure out how we make general employees more productive one of the common really common prompts engineering use cases we see as something around OK R and something around insights so we all do these every quarter right we have insight sessions with our management managers we create OK RC there for ourselves or for our teams or for our organizations and there are a lot of folks that were have been and still are trying to look at how we could have deep conversations with from a prompt engineering perspective on on actually trying to improve the ability for us to generate OKR's and and insights and make it a more productive conversation and using chats in order to do that prompt engineering is a fascinating example that that you see um and it's something that is probably worth noting here when somebody come first comes to a tool just the the tool like like a charge GBT or IGBT or any of the other implementations we have inside of Intel when they first see that they look at it and they recognize it they think they know how to use it they're like ohh you know what I've seen this before it's a search box it says ask me a question and so they think of it more like Google search or Bing search where they ask a question and they're generally quite disappointed with with the response that they get um and they will a lot of times walk away from the tool I can tell you that the platform that my team supports we've had about 17,000 people come to our tool over the last six the last two months of those 17 thousand 7000 have never come back so so they're missing something right why why are more why do I have you know 50,000 people coming back pretty regularly and 7000 who just left there's a reason and it has to do with prompt engineering and a lack of knowledge around prompt engineering and this is something that's really important to us in order to try to help people and figure out and get them inspired on how they could do prompt engineering because this is really where at least in my opinion we can see a lot of productivity gains in the company so prompt engineering is sort of one of those areas that's that's really you know kind of something we're digging into also you know you probably have read there's articles out there that will say prompt engineering is dead and it may die eventually right we may get to the point where we can really work with the intent of the user in a way that we can don't have to do any that's the big prompt engineering but for the time being it is here and it's one of the best ways that you can become more productive in using it so these three big common use cases the code generation developer productivity the rag and the QA bots and prompt engineering and general productivity these are kind of the things we saw during this this work group that we were running over the last year or so next slide So coming out of our journey from an IT perspective we decided to create a platform we're calling it the gas platform or the generative AI as a service platform and it we have a very specific thing that is motivating us based on what you have seen we are really focused on trying to hit that 30 to 40% productivity improvement that has been handed to us by our leadership trying to figure out how we can enable that faster and easier for teams and so we're looking at two key things one is reusable capabilities and the other is platform democratization so when you think about reusable capabilities you know it's it's not code if you will so we're not thinking about reasonable code we're actually thinking about big high level reusable capabilities if you break down any generative AI application that we built today at Intel it's generally made-up of the same big pieces or the same parts they all have an AI workflow engine in them whether it's an off the shelf one or open source one like lynching or semantic kernel or whether it's been constructed handcrafted they have that AI workflow in there that's the thing that kind of manages the combination of data and and prompting and an inference and embeddings right that whole kind of workflow of what you would do when you're building your application they all have that they all have to to some extent a data ingestion capability and a data ingestion pipeline so getting the data from an unstructured data source into a vector store so whatever they do they all kind of have that as well and they also have some sort of model inferencing so whether most apps that Intel are running on Azure open AI which is a little bit disheartening well I'll talk about that in a future slide but we really kind of have to shift that a little bit over time that's what we need to focus on but they all have that model inferencing layer that's in there somewhere so those are kind of the three big giant capabilities that people have they also kind of we think about it too they'll all have UI shouldn't they all do most of them have a UI all there most of them have or some of them have an API layer that they've exposed as well but they all kind of have those big buckets of of things right so from from our perspective in it the thought is why should people rebuild that every time do they need to rebuild that every time is that something that is easy enough for us to provide as a capability maybe we can just give you configuration and you can configure how things work you know why would a team want to rebuild something over and over again that's not the innovation right that's not the innovative stuff that we need business teams product teams and everyone else focusing on it's that is the standard out-of-the-box platform level stuff that everyone needs to do that's IT bread and butter we know how to do that we know how to take those platforms we know how to scale them that's what we do that's our job we build these enterprise platforms but that innovation the thought about what can I do to make what can I do to make my team more productive that is generally with the groups where they're innovating right now that one of the reasons they're building these custom platforms and these is that they are trying to innovate and they don't have a place to do that innovation so they're doing it in their own platform but they're rebuilding things over and over and over again which is very inefficient the next thing we're focused on is platform democratization and this will take me into my next slide but just at a high level we want everybody to be able to use the platform we want everybody whether how no matter how technical or non-technical they are we want them to be able to get value out of the platform whether they're just a user or whether there's a data scientist we wanted them to be able to use the platform so easy in it we see the real value of generative AI in in the small things that you do as teams every single day so there are to be certain huge programs out there that are going to return a lot of value in terms of productivity to the company big giant programs but we can only do so many of those they're very expensive to do we have a limited number of resources and all of this smaller tasks that you do on a regular basis there isn't going to be a big program to put those in place so how do we provide the tools to everyone so that they can configure in a very inexpensive way um their own assistance which is a concept I'll talk about too so we'll go to the next slide here what I'm going to talk about here is our an extension of the democratization that I I mentioned in the previous section and it is a platform what we're really driving towards which is a platform for everyone so if you think about different types of users we have our end users sure people that come to the platform they may want to do some chatting with it and they may want to discover what other people have done as well right what have you done what have you done is there an OK R assistant out there that I can go and grab is there something that can help me with my insights is there something that can help me make my benefits selection or any of the general things that you do as a person on a regular basis meeting summary right that's another one I need something to summarize my meetings and most of the platforms that have been built out there until they all have meeting summarization which is great um that there needs to be these sort of core things that people are going to want to do right I just want to use the tool then there's the power user the power user is somebody who can manipulate prompts create some engineered prompts tune some prompts for specific models and then we would take that and publish it into a marketplace so that other people could find it also included within this concept and this is what I mentioned before the term assistant this is really what we're we're starting to kind of you know work down on which is an assistant could be created by a power user a developer or a data scientist those are the remaining roles right and it can be used by an end user or any of the others in there as well obviously but an assistant contains the workflow it contains the prompts it contains data as well in order to help accomplish a specific task a specific ask a specific thing so these power users are tuning these prompts and creating things publishing in the marketplace creating assistance the developer role they're creating more of an advanced assistant if you will so maybe they are creating some bringing some plugins in for for Python so they may want to do some specific Python execution maybe they have some data that's in a enterprise system that they want to access maybe they have some data that is in an unstructured data that they have in SharePoint they want to bring all of this data together all of these capabilities together and they to configure a plan for the workflow engine to execute so when a developer comes in or sorry when the user comes into the assistant that they create that the AI workflow takes place under the covers they just have to configure it our plans are or as we look at this you know we we have different plans in this place but we are you know potentially going to allow people to just upload a yaml document or maybe it's adjacent document that's yaml in our dev systems right now and we'll be we'll be able to kind of configure how the system works the last role we have here is just the traditional AI and ML data engineer and data scientist they're still huge role for these folks obviously but these people will be able to fine tune models on the platform they'll be able to implement advanced rag pipelines as well should they should they need that so they're they're definitely groups out there they're going to need that capability and these people will be able to continue to play the valuable role that they do  Next Slide.At a high level this describes the three big components in our architecture and what and subsequently it's what we see in most other architectures as well over on the left we have data the data portion of the platform is really all about taking unstructured data and bringing it into the platform so that it can be used right we're taking the data and we're vectorizing it and then it can be used by the EA workflow tools that are there and we're talking about really any unstructured data that you have what we are providing on in the data side as well is the ability for people to just kind of select models like you're building an assistant and you want to bring in data and vectorize it you'd be able to just and you don't have a lot of technical skills you'd be able to select a preconfigured pipeline that would do you know mostly what you wanted you may have a specialized pipeline for word documents or for office docs and another pipeline that's out there for for web pages and things like that so depending on the kind of data you're bringing in we would allow people to kind of just select from drop downs on on what they wanted to do of course hitting back on that AI and mill engineer we will also have the ability for people to deploy custom pipelines into here so that they can ingest the data structured data is also very important as part of the data picture we have a lot of structured data sources at Intel we have a lot of master data we have a lot of enterprise data as well and I know there's every team has structured data out there and it's not always most efficient or effective to vectorize structure data could be um but we still need to make that available as structured data into the AI workflow engine and through so through a sets of plugins and you know into the air workflow tools we will be able to continue to or allow the access of to structured data via APIs into the into the a workflow tool going all the way over to the right on the picture we have large language models and and we all know what these do right they analyze the generate text and from our perspective right now a lot of that is done on a commercial cloud platform a lot of it's done with Azure open AI and all of that inferencing all of it is done with NVIDIA that's not a place we want to be I don't think that's good for Intel probably all don't either we really need to work out how we can run more of our workload more of our tentative our workload on our Intel hardware my team is working on that right now I know there's another team for sure that's working on that trying to figure out how do we get more and more of the workload running on on the gaudy AI accelerators as well as the Xeon data center processors so we're really focused on that but from a consumer perspective if you think about something consuming this architecture building things on this we don't want someone to rebuild that we want to be able to host that and just make those models available so you can then you know instead of using open AI GPT 4 you could use a different open source model than we might have 1 horseman gowdy still TBD on those so I'm not going to say which models for sure we'll have on there but we we're we're working through different options right now now back into the middle the a workflow is sort of the heart of the operation um we this is where um assistance sometimes called agents are executed to accomplish goals this is all we plan to make this all configurable we actually have it configurable right now and when you create something in the a workflow when you create an assistant or an agent in the a workflow engine we will automatically expose that via chat so that you can start working with it right away and through a process it can be exposed via as an API now if you think about kind of the tools that you're developing maybe you have chatter interfaces already maybe you have an application maybe you're not aren't gonna use the chat that we provide but we'll be have seen over and over again is that a lot of people need that chat interface and for advanced teams especially in advanced abetted IT teams that are out there they you can build what you want right you have the skills but the vast majority of people at Intel the vast majority of teams at Intel they don't have the type of skills that you have they they've heard of generative AI they know about generative AI they've seen what it's done maybe they've watched some videos read some articles but they need tools that they themselves can configure to accomplish goals without having to hire software engineers without having to engage it right these are all really expensive endeavors and so we're we're really trying to target them and give them the tooling that they need to to do that themselves and we believe that this architecture will fulfill that next slide.Taking that a little bit more into the architecture and then also specifically calling out Intel products I have this slide umm as I as I mentioned we'll kind of go down from the top here so as I mentioned before we will be having a universal chat interface in a marketplace that we make available to people client apps can create whatever they want call APIs they will all be invoking and using assistance if you aren't familiar with the concept of assistant it's similar to what Chad TPT has they just call them GPT uh Microsoft calls them assistance so conceptually it's just a preconfigured workflow AI workflow that includes both plugins to connect to external data as well as internal data as vectorized data as well as highly engineered prompts so it's really a plan prompts plugins code and things like that in order to accomplish a goal so all of those things they're callable the API we believe that many of the tools and plot and the things that are out there today could be recast as assistance we'll see how that goes overtime I'm pretty pretty confident that that's the case and then of course all of that sits on top of our generativity a service platform now we don't know how the Intel products are all going to fit in except one for sure that I'm absolutely positive we will be using gaudy in our platform we have it in POC mode right now I have access to the inference APIs I've been using them so we absolutely will be using gowdy hardware to to host open source large language models that is a for sure thing the other products out there though when it comes to like articulate or converge IO we are trying to figure out where they fit we're doing a sort of proof of technology or proof of concept with um with articulate right now in the TV space we're trying to learn learn how they how we could use that and how we can leverage it as well as we're working with with the Intel developer cloud folks to try to figure out how we can use that as well so yes we absolutely need to run on Intel hardware but at the same time we need to look at other Intel products too so we're kind of trying to figure out the right way to leverage those products and bring them into the platform so we don't obviously our capabilities that we're providing to to our clients and to our consumers wouldn't change so you know if for instance if we were talking about articulate that's a that's more of a retriever as a retrieval of the service capability we would implement that under the covers and people would be able to continue to use the assistant just like they have in the past to retrieve data we made slide in articulate depending on what we needed to do with it so trying to figure that out but it really we're trying to kind of figure that that whole thing out how do we properly leverage Intel capabilities where possible so this is something that we're really focused on as well next slide.Another thing that our platform does it allows us to do Co marketing opportunities so obviously we we we want to use into hardware but at the same time there may be opportunities in the future for Intel to do Co marketing I'm I you know we're trying to be flexible here in terms of giving giving opportunities to do different things but we we we want to target workload over to to Google or over to IBM or over to Azure or depending on depending on how things work out but we really want to be able to be very very flexible and choose really any cloud provider under the covers so we want to be able to target the right workload do the strength of each provider and potentially allow allow Co marketing should that ever that opportunity ever arise so we don't want to tie ourselves I think that's probably the important important thing here we're not going to tie ourselves to 1 vendor we are going to tie ourselves to Azure sure the open AI models are there but what about what what if what happens if if suddenly Google has a better product we want to be able to use that we want to be able to use the strength of each provider so it's really important for us not to tie ourselves to the them as well also as I mentioned previously we want to be able to enable and use the gaudy accelerators where we possibly can it really is important for us to use the entire hardware and and we want to provide as an IT organization we'd love to provide our sales teams with great proof points and great information and great examples of how organizations can leverage these tools next slide.All right our road map so not committed I lots of safe harbor language here we are we are focused on on delivering these capabilities but you know things change so you know in the legal portion of this presentation it mentions that you know we can't count on any of this but you know we're very much trying to March towards this but but once again it's definitely not a committed road map this is just a high level of what we're doing have this laid out on the top and in terms of releases this is an IT thing but we we have a quarterly road map as well which I'm happy to share but that this is kind of how we think of things in IT for better or for worse so in in April So what are we targeting to release we really are kind of focused on improving our knowledge assistance so giving people the ability to build start to build and construct their own assistance now those assistants could have data in them they could be what we are calling an inference assistant so we're where we allow people to inference through our platform so instead of going out and building your own connectivity to open AI your own express routes your own all your own stuff you should be able to make inference calls through through our platform and and and leverage some of the capabilities that we have there we are trying to give people the ability to share their prompts so as as you like say hey here's some prompts I wanna do give people the ability to share those I did mention earlier as well sharing prompts and giving people who've never used these tools the ideas about how to best gain value from prompts is super important sharing prompts is one way of doing that there could be some other way as well the high performance features we actually have implemented those so those who use our tool today which is igt.intel.com and I'll give an example below when we get there but those who use our tool today and we'll notice it performs quite well we're purchasing capacity committed performance capacity from Microsoft to do that we've also deployed our solution in a highly scalable way so um we're deployed globally so you know every zone at Intel we have a deployment in I think it's six if I remember right so 6 deployment zones you know couple on the West Coast Asia Europe Israel etcetera all deployed and so users around the company who use the tool should feel that it runs pretty well for them um yeah challenges with the with the open AI inferencing still exist in terms of where i"
# ]
# complex
# prompts = [
# "You are a wonderful assistant. The following information is an agument that I am making to use Microsoft Semantc Kernel instead of LangChain.  Can you review this information and reformat it so that it makes sense:  I will be setting up a meeting early next week to drive this decision to a close.  I want to go into that meeting with you and I aligned.  Let me know your thoughts here.Something I won’t say in the meeting, but it is important to contemplate.  As far as I know, we are the only team at Intel to thoughtfully select a GAI workflow library.  We evaluated several frameworks and chose Semantic Kernel based on more than our team’s current skillset.  I don’t think anyone else has done this.Decision Factors Capability.  We need to choose a library that provides us the capabilities to deliver generative AI workflow as a service.Scalability.  The platform needs to scale to tens of thousands of users and hour AND millions of API calls an hour.Supportability.  We need to be able to support the platform long term for all of Intel as an enterprise capability.Roadmap focused on Intel Hardware support for LLMs.  The platform must be able to showcase Intel hardware and demonstrate how Intel hardware can be used in a cost-effective way to enable generative AI.The argument for SK Capability o Semantic Kernel provides all of the required capabilities for a workflow library.  We can give developers and configurators complete control over the actions that the workflow takes or we can allow the library to leverage an LLM to provide planning capabilities.The library provides the ability to include any OpenAPI compliant API into a workflow with simple configuration as a plugin.  This allows us to extend the workflow library to utilize any of our thousands of APIs that we already have at Intel.o The plugin extensibility allows us to provide code based plugins that can enable various functions inside the workflow engine that LLMs are not good at doing.  Math, time, etc.o The plugin extensibility allows us to extend the workflow engine in a way that enables us to provide a callable Python runtime that can execute any Python code.o All other aspects of the library are easily extended using the standard plugin interface, including LLM providers, vector db providers, etc.  This gives us the ability to implement anything we want in the future without requiring us to modify the original open source solution from MSFT.o There is out of the box support for Microsoft’s services such as MS Graph.• Supportability o Semantic Kernel is built by Microsoft with the backing of their open source program.  Microsoft is very committed to building open source software and keeping it open source.  It is expected that they will provide support for SK for a long time.o Intel IT knows how to support .Net Core based projects.  We have done it for years.  We have many off the shelf libraries we can pull to perform basic tasks like authentication and authorization, logging, real-time monitoring, etc.o Intel IT has thousands of C# developers on staff and can staff augment with C# developers very easily should we need to move this platform into a sustain mode down the road.o Intel IT knows how to deliver containerized apps using this architecture on IT CaaS, Azure, or any other cloud provider.o Intel IT has a long relationship with Microsoft and has access to assistance for architecture questions and support questions.  We know where to go to get answers to our questions.o Microsoft has a well know security methodology for evaluating security bugs in their open source software and closing them.  There is a well established process for reporting security bugs for SK.• Scalability o Intel IT knows how to scale .Net Core platforms and solutions.  We do this all the time and can easily scale the platform to meet the needs of tens of thousands of users an hour and millions of API calls an hour.o Moving the execution of the Python runtime for teams to leverage outside of the core implementation ensures that we can control the resource required to operate the platform at scale.  We won’t lose control of the core kernel execution due to long running programs that hold execution threads.• Roadmap o While SK does not specifically enable implementation of our hardware, it does give us the ability to easily extend our GAI solution to use Intel Hardware via an MLOps platform.o Through our partnership with MSFT, we can contribute back to SK to enable others to use Intel Hardware.o Microsoft claims to be using, or will be going to use, SK as part of all of their co-pilot offerings. This will ensure continually enhancements to the library.o It is extremely clear what will delivered in upcoming releases of SK.  MSFT uses GitHub projects to communicate this with the community.The argument against Semantic Kernel• Capability o We don’t get everything we want off the shelf.  In order to enable Python code execution for instance, we would need to implement external code containers.  While this has a benefit of allowing us to isolate the workflow runtime away from running code, we still have to build and support this.o Semantic Kernel’s capabilities align with Microsoft’s Azure platform.  Microsoft is providing SK as a revenue generator for Azure and its other services.  There is a reason that you find out of the box integration with MS Graph and any vector db that is available in Azure.o If we want off the shelf support for other aspects of the platform, such as REST API serving, we need to build our own wrapper capabilities.• Supportability o The required software engineering role can be expensive if not resourced correctly.• Scalability o While we know how to scale .Net Core C# apps, we could run into problems when we scaled a SK based workflow app especially as we are forced to move some execution, for instance the execution of Python code, outside of the kernel execution runtime.• Roadmapo This is still an open source project that is new.  It doesn’t have a long running history of support.  Our guidelines in IT state that we shouldn’t use open source projects that are less than 3 years old.  I wrote the guidelines that way because operational support and roadmap of continual updates and fixes (most importantly security fixes) is critical for IT systems that live 5 – 10 years.o It is likely that MSFT will continue to tie more and more of their products into Semantic Kernel.  While this isn’t a huge concern for us, since Intel IT does lean as a MSFT shop, but it could make it difficult to change directions if we are not careful about what we implement.The argument for LangChain• Capability o LangChain provides all of the required capabilities for a workflow library.  We can give developers and configurators complete control over the actions that the workflow takes, or we can allow the library to leverage an LLM to provide planning capabilities.o The core library provides a vast number of features that have been added by open source contributors.  LLM providers, data sources, vector dbs, etc. people have added a ton of features.o The library provides the ability to include any OpenAPI compliant API into a workflow with simple configuration as a plugin.  This allows us to extend the workflow library to utilize any of our thousands of APIs that we already have at Intel.o We can provide our own python libraries that can perform tasks that LLMs are bad at like math, time, etc.o The library is easily extended so we could add our own capabilities without requiring us to change the LangChain core project code.o Additional capabilities like LangSmith are being provided by LangChain (the company) and will be available as cloud based services.• Supportability o LangChain is a pure Python offering that has the support of thousands of community members.o Python is a very popular language in both data science roles and software engineering roles.• Scalability o I believe that a Python based hosting solution can scale.• Roadmap o It is certainly possible for us to provide modifications to the library that will enable other people to use Intel Hardware.  We can certainly do this as well.  The library is highly extensible and built to allow us to do this.o Community members can contribute to the project in a safe way which can enable more capabilities in the future.The argument against LangChain• Capabilityo We don’t get everything we want off the shelf with LangChain.  We are going to need to build a few connectors that talk to our MLOps platform.  We are going to need to build a custom search connector for Acceler8.o As LangChain is driven by their VC funders to seek revenue, it is hard to determine the course of the capability.  It appears that additional capabilities like LangSmith are being added as cloud based services with cost.o The capabilities added are driven by the community.  This is a two edged sword.  The library has lots and lots of things we will never use,o It is hard to influence the community roadmap without contributing code ourselves.• Supportabilityo We must go to the community for answers to questions or for guidance when problems arise.o Intel IT does not have any off the shelf components to build scalable Python based API.  We would need to construct a new set of libraries to support basic functionality like logging, real-time monitoring, and authentication and authorization.o LangServe, the open source chain hosting platform for LangChain is an early project and would require significant “plumbing” work for Intel IT to make it work in their environment.o Some project teams have had problems moving from one version of LangChain to another because of lack of backward compatibility (this might be solved with the latest versions of the libraries where community contributors and isolated to a different project)o It is difficult to guess at the long term goals of LangChan (the company).  They are the stewards of the libraries, but are now funded by VC.  Those VC funders expect a return on their investment and it remains to be seen as to how they will get that.  The LangChain (the company) revenue stream is not clear to me.o This is a new open source library and the open source standards that I wrote state libraries should have at least a 3 year history before being used in IT projects.o We do not know if LangChain (the company) will address security vulnerabilities in the same way as other enterprises that steward open source projects. Scalability o I believe that we can scale Python, but I could not point to a person in Intel IT that has experience scaling a Python solution to the level required by Intel.  I am sure we can do this, but we will be breaking new ground to do it.• Roadmap o It isn’t clear what is being delivered by LangChain and when.o They are certainly adding new features like LangServe, but I am concerned that their latest feature LangSmith requires a license to use.  I am not sure how their VC investors expect to be compensated, and am concerned (without data) that the roadmap could lead to a tie in with LangChain (the company) product.Comparison Summary Capability: Semantic Kernel and LangChain provide us with almost exactly the same capabilities but come in different wrappers.  LangChain is a pure Python implementation (JS is on the way), and Semantic Kernel is a C# first implementation (with full support for Python and Java).Supportability: Intel IT knows how to support C# .Net Core based platforms very well.  Intel IT does not know how to support enterprise Python based platforms.  To meet supportability requirements, we must choose Semantic Kernel.Scalability:  Intel IT knows how to scale C# .Net Core based platforms.  We do it all the time.  We do not know how to scale Python based platforms.  We certainly can scale Python based solutions, but it will take time., To meet scalability goals in the time we have, we must choose Semantic Kernel.Roadmap:  Both products offer us the ability to implement on our own hardware.  There is no direct tie between the workflow library and an Intel hardware solution.  We have to build that using either library.  Because of the skillset involved in doing this work, we need to choose Sematic Kernel to meet our timeline goals.I have reviewed your argument and reformatted it for clarity and coherence. Here's the revised version:I am planning a meeting next week to finalize our decision on the Generative AI (GAI) workflow library. I believe we should align on this before the meeting. As far as I'm aware, we are the only team at Intel that has thoroughly evaluated several frameworks and chosen Microsoft's Semantic Kernel (SK) over LangChain. This decision is not solely based on our team's current skillset.Our decision is based on four factors:Capability: We need a library that can deliver generative AI workflow as a service.Scalability: The platform must scale to tens of thousands of users and millions of API calls per hour.Supportability: We need a platform that we can support long-term across Intel as an enterprise capability.Roadmap: The platform must showcase Intel hardware and demonstrate its cost-effective use in generative AI.Here are the arguments for and against both SK and LangChain:Semantic Kernel (SK) Pros:Capability: SK provides all required capabilities for a workflow library. It allows us to extend the workflow library to utilize any of our APIs at Intel and provides out-of-the-box support for Microsoft’s services.Supportability: SK is backed by Microsoft's open source program, and Intel IT has a long relationship with Microsoft. We have many off-the-shelf libraries and thousands of C# developers on staff.Scalability: Intel IT has experience scaling .Net Core platforms and solutions.Roadmap: SK allows us to extend our GAI solution to use Intel Hardware via an MLOps platform. Microsoft plans to use SK in all their co-pilot offerings, ensuring continual enhancements.Cons:Capability: We would need to implement external code containers to enable Python code execution. SK's capabilities align with Microsoft’s Azure platform, which could limit our options.Supportability: The required software engineering role can be expensive if not resourced correctly.Scalability: We could face problems when scaling a SK-based workflow app.Roadmap: SK is a new open source project with less than 3 years of history, which goes against our IT guidelines.LangChain Pros:Capability: LangChain provides all required capabilities for a workflow library and has a vast number of features added by open source contributors.Supportability: LangChain is a pure Python offering with the support of thousands of community members.Scalability: A Python-based hosting solution can scale.Roadmap: We can provide modifications to the library that will enable others to use Intel Hardware.Cons:Capability: We would need to build a few connectors to fully utilize LangChain. The library's capabilities are driven by the community, which could limit our influence.Supportability: We must rely on the community for support. Intel IT does not have any off-the-shelf components to build scalable Python-based API.Scalability: We lack experience in scaling a Python solution to the level required by Intel.Roadmap: It isn’t clear what is being delivered by LangChain and when.Comparison SummaryBoth SK and LangChain offer similar capabilities but in different formats. However, considering supportability, scalability, and roadmap, Semantic Kernel is the more suitable choice for us due to our familiarity with C# .Net Core based platforms and our long-standing relationship with Microsoft.I want to reformat this as a persuasive essay that favors Semantic Kernel over LangChain.Title: The Case for Semantic Kernel over LangChain for Generative AI Workflow Library Introduction:As we approach our upcoming meeting to finalize the decision on the Generative AI (GAI) workflow library, it is crucial to align our perspectives. Our team at Intel stands unique in its thoughtful selection of a GAI workflow library, having evaluated several frameworks and chosen Microsoft's Semantic Kernel (SK) over LangChain. This decision extends beyond our team's current skillset and is based on four key factors: capability, scalability, supportability, and a roadmap focused on Intel Hardware support for LLMs. Body:The Argument for Semantic Kernel:SK offers robust capabilities as a workflow library, providing developers and configurators with complete control over the actions that the workflow takes. It allows us to extend the workflow library to utilize any of our APIs at Intel and provides out-of-the-box support for Microsoft’s services.In terms of supportability, SK is backed by Microsoft's open source program, and Intel IT has a long relationship with Microsoft. We have many off-the-shelf libraries and thousands of C# developers on staff. Intel IT has experience scaling .Net Core platforms and solutions, which aligns with our scalability needs.The roadmap for SK is promising. While SK does not specifically enable implementation of our hardware, it does give us the ability to easily extend our GAI solution to use Intel Hardware via an MLOps platform. Microsoft plans to use SK in all their co-pilot offerings, ensuring continual enhancements.The Argument Against Semantic Kernel:Despite its strengths, SK does have some limitations. We would need to implement external code containers to enable Python code execution. SK's capabilities align with Microsoft’s Azure platform, which could limit our options. The required software engineering role can be expensive if not resourced correctly. We could face problems when scaling a SK-based workflow app. SK is a new open source project with less than 3 years of history, which goes against our IT guidelines.The Argument for LangChain:LangChain also provides all required capabilities for a workflow library and has a vast number of features added by open source contributors. It is a pure Python offering with the support of thousands of community members. A Python-based hosting solution can scale. We can provide modifications to the library that will enable others to use Intel Hardware.The Argument Against LangChain:LangChain has its own set of challenges. We would need to build a few connectors to fully utilize LangChain. The library's capabilities are driven by the community, which could limit our influence. We must rely on the community for support. Intel IT does not have any off-the-shelf components to build scalable Python-based API. We lack experience in scaling a Python solution to the level required by Intel. It isn’t clear what is being delivered by LangChain and when.Conclusion:In comparing Semantic Kernel and LangChain, both offer similar capabilities but in different formats. However, considering supportability, scalability, and roadmap, Semantic Kernel is the more suitable choice for us due to our familiarity with C# .Net Core based platforms and our long-standing relationship with Microsoft. Therefore, to meet our capability, supportability, scalability, and roadmap goals within our timeline, we should opt for Semantic Kernel.With your updated knowledge base, summarize this conversation into one slide that I can use for an architecture community.Slide Title: Choosing the Right GAI Workflow Library: Semantic Kernel vs. LangChain Slide Content: Objective: Finalize decision on Generative AI workflow library for IntelKey Decision Factors:Capability: Deliver GAI workflow as a service Scalability: Support tens of thousands of users & millions of API calls/hour Supportability: Long-term enterprise capability for Intel Roadmap: Showcase & optimize Intel hardware for LLMs Semantic Kernel (SK) - Microsoft: Pros:Comprehensive capabilities for workflow control and extensibility Strong supportability with .Net Core expertise & Microsoft partnership Proven scalability within Intel IT infrastructure Clear roadmap with ongoing enhancements & Intel hardware integration potential Cons:Requires additional development for Python execution support Newer open source project (<3 years), potential risk for long-term support LangChain: Pros:Extensive capabilities with community-driven features Python-based, popular in data science and software engineering Cons:Requires development of new connectors and support libraries Unclear long-term roadmap and scalability within Intel IT Recommendation:Choose Semantic Kernel for its alignment with Intel's supportability, scalability, and strategic roadmap, leveraging our existing C# .Net Core expertise and Microsoft relationship.Next Steps:Align on SK decision in upcoming meetingPlan for implementation and scaling strategy Note: Slide should be visually balanced, with clear headings, bullet points, and a simple color scheme that highlights the recommendation. Use of Intel and Microsoft logos may be appropriate to visually represent the partnership and existing infrastructure."
# ]

# prompts = [
#    "You are a wonderful assistant. can you please summarize the below content? Create meeting minutes from the VTT included within this message.  If no VTT is included, then guide the user through the process of downloading and opening a VTT which was recorded with Microsoft Teams and stop and wait for the user to input the VTT data before proceeding with the rest of the directions.  Create a table of assignments that were made during the meeting that has the assignment and the person who owns completing the assignment.  You are an expert meetings minute assistant.  Create a summary section that summarizes the intent of the meeting in less than 1000 words. Include summaries of major topics that were discussed in less than 1000 words. Under this message insert this text “Continue chatting with your meeting using these generated prompts:” and provide 3 additional prompts that could be executed in order to provide additional insights into the meeting. The additional prompts should appear at the end and be in italic text.  Use a clear and professional tone.  Do not mention VTT or transcripts in the meeting minutes   Do not mention VTT or transcripts in the meeting minutes.  WEBVTT FILE   ﻿1   00:00:00.000 --> 00:00:07.090   [ INTENSE MUSIC ]   2   00:00:07.090 --> 00:00:11.386   -We have lock, and are good to send that command.   We have thirty-one minutes and thirty-two   3   00:00:11.386 --> 00:00:15.432   seconds for our support.   4   00:00:15.432 --> 00:00:18.601   Go for status buffer dump.   5   00:00:18.601 --> 00:00:22.605   -What you're looking at with a telescope, of   course, is the light from billions of light-years   6   00:00:22.605 --> 00:00:27.152   away. So the further you look, the more   you're going back towards the Big Bang and   7   00:00:27.152 --> 00:00:30.905   understanding how the universe was formed.   8   00:00:30.905 --> 00:00:36.327   -The Webb telescope will be groundbreaking   because it has capabilities that are different   9   00:00:36.327 --> 00:00:39.080   than the Hubble Space Telescope.   10   00:00:39.080 --> 00:00:43.710   -We have equipment that is so much more powerful   than anything we've ever had before, that   11   00:00:43.710 --> 00:00:47.797   it's almost impossible to tell   what we will discover.   12   00:00:47.797 --> 00:00:52.969   Hubble Eye In The Sky   13   00:00:52.969 --> 00:00:59.100   Episode 3: Time Machines   14   00:00:59.100 --> 00:01:01.895   -Hubble's accomplishments include something   15   00:01:01.895 --> 00:01:03.938   called the deep fields.   16   00:01:03.938 --> 00:01:09.903   Looking out into space and collecting   light, sometimes for many days,   17   00:01:09.903 --> 00:01:12.489   these deep fields have revealed visually to us,   18   00:01:12.489 --> 00:01:18.661   a universe absolutely teaming with galaxies,   hundreds of billions of galaxies.   19   00:01:22.165 --> 00:01:24.626   -One of the neat things about the Ultra Deep Field,   20   00:01:24.626 --> 00:01:26.544   and one of the things that made it so unique,   21   00:01:26.544 --> 00:01:28.254   was how long it took us   22   00:01:28.254 --> 00:01:31.382   to take that image.   There's an exposure time that's expressed,   23   00:01:31.382 --> 00:01:36.346   I think it's 11.2 days. It’s a very, very   long exposure time, but probably what's more   24   00:01:36.346 --> 00:01:42.936   important is how many orbits it took us to   do that. 400 orbits of Hubble data to take   25   00:01:42.936 --> 00:01:47.232   that image. You only get   15 orbits a day. To take 400   26   00:01:47.232 --> 00:01:53.363   orbits and say we're going to observe this   one spot in the sky for 400 orbits, and the   27   00:01:53.363 --> 00:01:58.952   really unique thing about that was they picked   a spot where there wasn't anything. They looked   28   00:01:58.952 --> 00:02:03.873   and they said there's absolutely nothing here.   And they said, you want to spend 400 Hubble   29   00:02:03.873 --> 00:02:08.711   orbits looking at nothing? And they said yes,   because we want to see what it can see. And   30   00:02:08.711 --> 00:02:18.721   I think the results from the science, I mean   it was amazing. What they saw was spectacular.   31   00:02:18.721 --> 00:02:21.516   -Hubble had spent two weeks taking pictures of empty places   32   00:02:21.516 --> 00:02:23.893   in the sky. And they saw they weren't empty at all   33   00:02:23.893 --> 00:02:26.271   there were thousands and thousands of galaxies.   34   00:02:28.606 --> 00:02:31.818   -We were amazed how many galaxies we found,   35   00:02:31.818 --> 00:02:33.653   and we continued to go back to that   36   00:02:33.653 --> 00:02:40.410   portion of the sky to increase that visibility.   37   00:02:40.410 --> 00:02:47.292   -The Hubble Space Telescope is an outstanding   time machine.   38   00:02:47.292 --> 00:02:51.713   It's incredibly important for our studies   with the Hubble Space Telescope to realize   39   00:02:51.713 --> 00:02:58.261   that when we're looking at a galaxy, we're   seeing it as it was millions of years ago,   40   00:02:58.261 --> 00:03:03.266   sometimes billions of years ago. It's taken that   long for the light to get to us.   41   00:03:03.266 --> 00:03:05.268   -What you're looking at with a telescope, of course,   42   00:03:05.268 --> 00:03:08.229   is the light from billions of light-years away.   43   00:03:08.229 --> 00:03:10.815   So the further you look, the more you're going back towards   44   00:03:10.815 --> 00:03:16.196   the Big Bang and understanding how the universe was formed.   45   00:03:16.196 --> 00:03:21.659   -What Hubble has revealed is that the universe   has in fact changed over these billions of   46   00:03:21.659 --> 00:03:28.791   years of time. The early galaxies, the very   distant ones as we see them, are simple. Sometimes   47   00:03:28.791 --> 00:03:33.504   they're messy looking, they're small. They   haven't had time yet to form that grand   48   00:03:33.504 --> 00:03:38.218   spiral structure.   Over time, we see galaxies actually merging   49   00:03:38.218 --> 00:03:43.181   with other galaxies and growing bigger and   bigger, and those mergers can look like train   50   00:03:43.181 --> 00:03:44.807   wrecks in our Hubble images.   51   00:03:51.731 --> 00:03:55.485   -These very, very deep exposures that Hubble   has been able to take, we have seen right to   52   00:03:55.485 --> 00:04:01.199   the edge of the universe, thirteen and a half billion years.   53   00:04:01.199 --> 00:04:04.702   When Hubble was first designed and envisioned,   it was never thought it could actually see   54   00:04:04.702 --> 00:04:09.415   that far out. But because of the advances   in the instruments that we've been able to   55   00:04:09.415 --> 00:04:13.711   put up on the telescope, and also the cleverness   of the scientists, they've come up with very   56   00:04:13.711 --> 00:04:18.007   interesting observing scenarios, doing these   really deep exposures, where we just sit there   57   00:04:18.007 --> 00:04:22.929   for orbit after orbit, after orbit gathering   the photons, we’ve been able to push Hubble   58   00:04:22.929 --> 00:04:26.933   out very, very far.   59   00:04:26.933 --> 00:04:33.606   -As Hubble looks out into these fields of galaxies,   we sometimes see clusters of galaxies. These   60   00:04:33.606 --> 00:04:38.945   are galaxies that are held nearby each other   by their mutual gravity.   61   00:04:38.945 --> 00:04:46.869   These clusters are massive conglomerations.   There's so much mass that they have an actual   62   00:04:46.869 --> 00:04:54.919   observable impact on space-time itself.   Einstein predicted that mass distorts space,   63   00:04:54.919 --> 00:04:59.215   but we didn't realize we could actually see   the effects of that. But with Hubble, we have   64   00:04:59.215 --> 00:05:05.513   been able to see distortions in space around   clusters of galaxies. The way we see that   65   00:05:05.513 --> 00:05:11.769   is when light from a background galaxy travels   through that cluster of galaxies, or around   66   00:05:11.769 --> 00:05:17.400   it, due to this gravitational lensing effect.   The lensing also magnifies that background   67   00:05:17.400 --> 00:05:22.947   galaxy, so if we look in some of these distorted   arcs, we can see more detail than we would   68   00:05:22.947 --> 00:05:28.453   ever have been able to see without gravitational   lensing, nature’s boost.   69   00:05:32.290 --> 00:05:36.377   -There are observations where we're explicitly   looking for the lensing and   70   00:05:36.377 --> 00:05:39.547   we're getting science out of that just otherwise   71   00:05:39.547 --> 00:05:43.718   would just not be doable. Hubble has really taken that   72   00:05:43.718 --> 00:05:47.096   to a next level. It's doing large amounts   73   00:05:47.096 --> 00:05:50.433   of astrophysics that it's just never been able to do before.   74   00:05:55.313 --> 00:06:01.069   -Some of what we're doing with Hubble is to   prepare for the new James Webb telescope,   75   00:06:01.069 --> 00:06:07.075   which we anticipate launching in 2021, which   will be able to see farther into the infrared   76   00:06:07.075 --> 00:06:14.207   part of the electromagnetic spectrum. That   enables us to see some galaxies that are difficult   77   00:06:14.207 --> 00:06:20.797   for Hubble to see because they're so far away   that their light is traveling through us through   78   00:06:20.797 --> 00:06:26.803   expanding space and gets stretched out into   redder wavelengths, often far into the infrared   79   00:06:26.803 --> 00:06:32.308   part of the spectrum. Even sometimes beyond   what Hubble is able to detect well. The Webb   80   00:06:32.308 --> 00:06:38.064   telescope will give us more information about   some of those very distant galaxies.   81   00:06:38.064 --> 00:06:43.444   -The James Webb Space Telescope is the follow-on   telescope after the great Hubble telescope.   82   00:06:43.444 --> 00:06:47.532   It extends the discoveries of Hubble into   the infrared spectrum region.   83   00:06:49.450 --> 00:06:53.371   We think that the first objects that grew   out of the Big Bang material probably happened   84   00:06:53.371 --> 00:06:58.960   in about a hundred million years after the   start. And we think the Webb telescope can   85   00:06:58.960 --> 00:07:04.173   pick them up. They're rare, they're hard to   find, but they should be there.   86   00:07:04.173 --> 00:07:08.678   The farthest we've been able to see with   the Hubble telescope goes back about 600 - 800   87   00:07:08.678 --> 00:07:13.891   million years after the expansion began, so   we think we get much, much closer to the   88   00:07:13.891 --> 00:07:18.187   first objects with the Webb telescope.   89   00:07:18.187 --> 00:07:20.982   -Hubble gives information that the Webb telescope   cannot give about   90   00:07:20.982 --> 00:07:24.861   visible and ultraviolet emission from things in the universe,   91   00:07:24.861 --> 00:07:26.821   and when we have all of that information coming in   92   00:07:26.821 --> 00:07:32.577   at the same time, it's like a banquet   of scientific return.   93   00:07:32.577 --> 00:07:37.165   -Now when we get the complete picture of every   wavelength you can possibly see from ultraviolet   94   00:07:37.165 --> 00:07:40.168   to infrared, we hope to have   the story of the growth   95   00:07:40.168 --> 00:07:42.795   of the first galaxies from the primordial material.   96   00:07:42.795 --> 00:07:46.632   So that will be a huge accomplishment   that depends on both pieces   97   00:07:46.632 --> 00:07:51.971   of equipment, the Hubble telescope and the   James Webb telescope working together.   98   00:07:51.971 --> 00:07:56.058   -So, astronomers are very excited about this   probability that we'll have both the Hubble   99   00:07:56.058 --> 00:08:01.022   Space Telescope and the Webb telescope operating   at the same time for quite a few years. That   100   00:08:01.022 --> 00:08:04.901   will give us an abundance of new understanding   about the universe.   101   00:08:04.901 --> 00:08:11.449   And already right now with Hubble, we're doing   preparatory observations for the Webb telescope.   102   00:08:11.449 --> 00:08:16.329   We're using Hubble to do things, for example,   like surveying distant galaxies to find out   103   00:08:16.329 --> 00:08:20.958   which ones would be prime targets for the   Webb telescope.   104   00:08:20.958 --> 00:08:27.590   In fact, scientists around the world are proposing   observations with Hubble right now specifically   105   00:08:27.590 --> 00:08:32.553   to help us learn information that will be   useful for making the best use of the Webb   106   00:08:32.553 --> 00:08:39.435   telescope as soon as it's launched and gets   going in its science observations.   107   00:08:39.435 --> 00:08:43.231   -I think the Hubble telescope has been the   most productive science instrument ever built.   108   00:08:43.272 --> 00:08:47.193   In astronomy, there's what we knew before   Hubble, and now, there's what we know after   109   00:08:47.193 --> 00:08:48.486   Hubble. They're so different.   110   00:08:48.486 --> 00:08:52.698   Of course, Hubble has now had a life of 30   years, so it's had a long time to make this   111   00:08:52.698 --> 00:08:57.912   revolution happen. So it's not all at once.   It's a gradual revolution, but it's still   112   00:08:57.912 --> 00:09:00.915   a huge revolution.   113   00:09:00.915 --> 00:09:05.461   Knowledge has changed dramatically over the   30 years of life of the Hubble telescope.   114   00:09:05.461 --> 00:09:09.549   so you couldn't even have imagined when the   Hubble was launched that we would have the   115   00:09:09.549 --> 00:09:13.135   wonderful cameras and spectrometers that we   fly today.   116   00:09:13.135 --> 00:09:17.306   We figured out how to send astronauts, we   trained the astronauts, we figured out what   117   00:09:17.306 --> 00:09:20.893   instruments could be put in. We figured out   how to repair everything that went wrong on   118   00:09:20.893 --> 00:09:25.189   the Hubble, and it's still alive today, 30   years after launch.   119   00:09:25.189 --> 00:09:30.027   I am so thrilled to say that our people were   able to do that.   120   00:09:30.027 --> 00:09:34.115   That's the operations team that makes this   possible. It's a miracle as far as I'm concerned,   121   00:09:34.115 --> 00:09:36.784   because it didn't have to be that way, but   they made it happen.   122   00:09:36.784 --> 00:09:44.208   Hubble Eye In The Sky   123   00:09:44.208 --> 00:09:51.173   [ INTENSE MUSIC ]"
# ]
# contents = []

# print("length of prompts: ",  len(prompts[0]))
# l = 512
# splited_prompts = []
# sum = 0
# while sum < len(prompts[0]):
#     splited_prompts.append(prompts[0][sum:sum+l])
#     sum += l

# prompts = splited_prompts

new_prompts = []

for i in range(4):
    # content = {"prompt": prompts[i%len(prompts)], "stream": False, "max_tokens": 4096, "best_of": 2, "use_beam_search": True, "temperature": 0}
    # content = {"prompt": prompts[i%len(prompts)], "stream": False, "max_tokens": 4096}
    # contents.append(content)
    new_prompts.append(prompts[i % len(prompts)])
######################################
# new_prompts.clear()
# new_prompts.append(prompts[2])


# Create an LLM.
# llm = LLM(model="facebook/opt-125m", device="cpu", quantization="awq")
# llm = LLM(model="TheBloke/Llama-2-7b-Chat-AWQ", device="cpu", quantization="AWQ")
# llm = LLM(model="meta-llama/Llama-2-7b-chat-hf", device="cpu", quantization="ns")
# TODO verify block_size >= max_model_len
# TODO set VLLM_CPU_KVCACHE_SPACE to X (GB) so that VLLM_CPU_KVCACHE_SPACE/(block_size*element_size) = num_cpu_blocks <= max_num_seqs. Otherwise, native kv cache may run out of slots.
ctx_size = 4096
llm = LLM(
    # model="meta-llama/Meta-Llama-3-8B-Instruct",
    model="meta-llama/Llama-2-7b-chat-hf",
    device="cpu",
    max_num_seqs=8,
    block_size=ctx_size,
    max_model_len=ctx_size,
    quantization="ns",
)
# llm = LLM(model="meta-llama/Llama-2-7b-chat-hf", device="cpu")
# Generate texts from the prompts. The output is a list of RequestOutput objects
# that contain the prompt, generated text, and other information.
for i in range(16):
    t0 = perf_counter()
    outputs = llm.generate(new_prompts, sampling_params)
    total_time = perf_counter() - t0
    # Print the outputs.
    total_prompts = 0
    total_generated = 0
    for output in outputs:
        prompt = output.prompt
        generated_text = output.outputs[0].text
        total_prompts += len(prompt.split(" "))
        total_generated += len(generated_text.split(" "))
        # print(f"Prompt: {prompt!r}, Generated text: {generated_text!r}")

    print(total_time, total_prompts, total_generated)
    print(f"prompts per second: {total_prompts/total_time}")
    print(f"tokens per second: {total_generated/total_time}")
