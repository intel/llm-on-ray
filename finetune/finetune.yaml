General:
  base_model: EleutherAI/gpt-j-6b
  gpt_base_model: true
  output_dir: /tmp/llm-ray/output
  checkpoint_dir: /tmp/llm-ray/checkpoint
  config:
    trust_remote_code: false
    use_auth_token: null
  lora_config:
    task_type: CAUSAL_LM
    r: 8
    lora_alpha: 32
    lora_dropout: 0.1
Dataset:
  train_file: examples/data/sample_finetune_data_small.jsonl
  validation_file: null
  validation_split_percentage: 5
Training:
  optimizer: AdamW
  batch_size: 2
  epochs: 3
  learning_rate: 1.0e-05
  lr_scheduler: linear
  weight_decay: 0.0
  device: CPU
  num_training_workers: 2
  resources_per_worker:
    CPU: 32
  accelerate_mode: CPU_DDP

deepspeed_config:
  fp16:
    enabled: auto
    hysteresis: 2
    initial_scale_power: 16
    loss_scale: 0
    loss_scale_window: 1000
    min_loss_scale: 1
  gradient_accumulation_steps: 1
  gradient_clipping: auto
  optimizer:
    params:
      adam_w_mode: true
      lr: auto
      torch_adam: true
      weight_decay: auto
    type: AdamW
  scheduler:
    params:
      warmup_max_lr: auto
      warmup_min_lr: auto
      warmup_num_steps: auto
    type: WarmupLR
  steps_per_print: 2000
  train_batch_size: auto
  train_micro_batch_size_per_gpu: auto
  wall_clock_breakdown: false
  zero_optimization:
    allgather_bucket_size: 2.0e8
    allgather_partitions: true
    contiguous_gradients: true
    offload_optimizer:
      device: cpu
      pin_memory: true
    overlap_comm: true
    reduce_bucket_size: auto
    reduce_scatter: true
    stage: 2

